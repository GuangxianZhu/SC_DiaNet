{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 9])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# load patches data from files\n",
    "train_images_patches = np.load('data/train_8x8_3x3.npy')\n",
    "test_images_patches = np.load('data/test_8x8_3x3.npy')\n",
    "train_images_patches = torch.from_numpy(train_images_patches)\n",
    "test_images_patches = torch.from_numpy(test_images_patches)\n",
    "\n",
    "# get the label from datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_offtrain = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_offtest = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_labels = [label for _, label in mnist_offtrain]\n",
    "test_labels = [label for _, label in mnist_offtest]\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# make them to be PyTorch tensors, and dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images_patches, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_images_patches, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# test dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "\n",
    "patch_size = 3*3\n",
    "patch_num = 8*8\n",
    "patch_out = 1\n",
    "main_head = patch_num * patch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0439, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def write_debug(header:str, debug_npy:np.array):\n",
    "    # convert numpy array to string\n",
    "    debug_str = ''\n",
    "    for i in range(debug_npy.shape[0]):\n",
    "        for j in range(debug_npy.shape[1]):\n",
    "            debug_str += str(debug_npy[i][j])[0:5] + ' '\n",
    "        debug_str += '\\n'\n",
    "        \n",
    "    # write debug_str to debug.txt\n",
    "    with open('debug/debug.txt', 'a') as f:\n",
    "        f.write(header+': \\t'+debug_str + '\\n')\n",
    "        \n",
    "# clear debug.txt\n",
    "with open('debug/debug.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "class Script_DiaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_num, out_num, device='cpu'):\n",
    "        super(Script_DiaNet, self).__init__()\n",
    "        self.log = False\n",
    "        \n",
    "        self.red_depths = list(range(2, 1000))\n",
    "        self.red_fulls = [sum(range(1, n+2)) for n in self.red_depths]\n",
    "        self.red_heads = [(x * 2) + 1 for x in self.red_depths]\n",
    "\n",
    "        self.red_full = -1\n",
    "        self.red_dep = -1\n",
    "        self.red_triangle = []\n",
    "        self.jump_weights = nn.ParameterList()\n",
    "        self.layers, self.masks = self.gen_layers(inp_num, out_num)\n",
    "        # each mask to device\n",
    "        for i in range(len(self.masks)):\n",
    "            self.masks[i] = self.masks[i].to(device)\n",
    "        # each layers multiply mask\n",
    "        self.x_hidden = []\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].weight.data = self.layers[i].weight.data * self.masks[i]\n",
    "            self.x_hidden.append(-1) # init x_hidden\n",
    "        \n",
    "    \n",
    "    def select_insert(self, n, sel_len) -> list:\n",
    "        assert (((n - 1) // 2) % 2) == (sel_len % 2), \"Invalid selection length\"\n",
    "        numbers = range(n)\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        center_index = len(odd_numbers) // 2\n",
    "        selected_numbers = odd_numbers[center_index - sel_len // 2: center_index + sel_len // 2 + (sel_len % 2)]\n",
    "        return selected_numbers if sel_len != 1 else selected_numbers\n",
    "        \n",
    "    def gen_layers(self, inp_num, out_num):\n",
    "        # create Red triangle(input triangle)\n",
    "        masks_list = []\n",
    "        self.bn_list = [] # batch norm list\n",
    "\n",
    "        # select input triangle by input num\n",
    "        red_dep, red_head = -1, -1\n",
    "        for i in range(len(self.red_fulls)-1):\n",
    "            if inp_num > self.red_fulls[i] and inp_num <= self.red_fulls[i+1]:\n",
    "                red_full = self.red_fulls[i+1]\n",
    "                red_dep = self.red_depths[i+1]\n",
    "                red_head = self.red_heads[i+1]\n",
    "                if self.log: print(\"red_dep:\", red_dep, \"red_head:\", red_head)\n",
    "                break\n",
    "        assert (red_dep>0)and(red_head>0), \"red_dep {} or head {} is not valid\".format(red_dep, red_head)\n",
    "        self.red_dep = red_dep\n",
    "        self.red_full = red_full\n",
    "        for i in range(red_dep, 0, -1):\n",
    "            if i == red_dep:\n",
    "                self.red_triangle.append(i*2+1)\n",
    "            else:\n",
    "                self.red_triangle.append(i)\n",
    "\n",
    "        \n",
    "        # create blue triangle(output triangle)\n",
    "        nn_output = out_num + (out_num-1)\n",
    "        blue_dep = int(nn_output/2-0.5)\n",
    "        if self.log:print(\"blue_dep:\", blue_dep)\n",
    "\n",
    "        total_dep = red_dep + blue_dep\n",
    "        if self.log:print(\"total_dep:\", total_dep)\n",
    "\n",
    "        # expand times\n",
    "        expand_times = total_dep - red_dep\n",
    "        if self.log:print(\"expand_times:\", expand_times)\n",
    "\n",
    "        # create expand time nn layer\n",
    "        expand_list = []\n",
    "        expand_list.append(nn.Linear(red_head, red_head-2, bias=False)) # first layer\n",
    "        self.bn_list.append(nn.BatchNorm1d(red_head-2))\n",
    "        self.jump_weights.append(nn.Parameter(torch.randn(1, red_head-2))) # Not in use\n",
    "\n",
    "        masks_list.append(self.fst_mask(red_head, red_head-2))\n",
    "        nn_in = red_head-2\n",
    "        for i in range(expand_times):\n",
    "            expand_list.append(nn.Linear(nn_in, nn_in+2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in+2))\n",
    "            masks_list.append(self.exp_mask_v2(nn_in, nn_in+2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in += 2\n",
    "        if self.log:print(\"expand_list:\", expand_list)\n",
    "\n",
    "        # shrink times\n",
    "        shrink_times = total_dep - blue_dep - 1\n",
    "        # if self.log:print(\"shrink_times:\", shrink_times)\n",
    "        shrink_list = []\n",
    "        for i in range(shrink_times):\n",
    "            shrink_list.append(nn.Linear(nn_in, nn_in-2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in-2))\n",
    "            masks_list.append(self.shr_mask_v2(nn_in, nn_in-2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in -= 2\n",
    "        if self.log:print(\"shrink_list:\", shrink_list)\n",
    "\n",
    "        # combine expand and shrink list\n",
    "        layer_list = expand_list + shrink_list\n",
    "        layers = nn.Sequential(*layer_list)\n",
    "        assert len(masks_list) == len(layer_list), \"masks_list {} and layer_list {} is not same\".format(len(masks_list), len(layer_list))\n",
    "\n",
    "        return layers, masks_list\n",
    "    \n",
    "    def fst_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"fst_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                start_idx = i\n",
    "                end_idx = i+2 + 1\n",
    "                mask[i, start_idx:end_idx] = 1\n",
    "        return mask\n",
    "    \n",
    "    def exp_mask(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-4)\n",
    "                b = min(max(0, i-2), in_dim-1)\n",
    "                c = min(max(0, i-1), in_dim-1)\n",
    "                d = min(max(0, i), in_dim-1)\n",
    "                e = min(max(0, i+2), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "    \n",
    "    def shr_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-2)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def exp_mask_v2(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-6)\n",
    "                b = min(max(0, i-4), in_dim-1)\n",
    "                c = min(max(0, i-2), in_dim-1)\n",
    "                d = min(max(0, i-1), in_dim-1)\n",
    "                e = min(max(0, i), in_dim-1)\n",
    "                f = min(max(0, i+2), in_dim-1)\n",
    "                g = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e,f,g]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def shr_mask_v2(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a_ = max(0, i-4)\n",
    "                a = min(max(0, i-2), in_dim-1)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                e_ = min(max(0, i+6), in_dim-1)\n",
    "                idx_lis = [a_,a,b,c,d,e,e_]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        container = torch.zeros((x.shape[0], self.red_full)) # (b, 28)\n",
    "        container[:, 0:x.shape[1]] = x\n",
    "        # split x for red triangle, froexample, 25-> 13,5,4,3,2,1\n",
    "        x_seg = []\n",
    "        slice_idx = 0\n",
    "        for i in range(self.red_dep, 0, -1):\n",
    "            if i == self.red_dep:\n",
    "                x_seg.append(container[:, 0:i*2+1]) # 13\n",
    "                slice_idx += i*2+1\n",
    "            else:\n",
    "                x_seg.append(container[:, slice_idx:slice_idx+i]) # 13:18, 18:22, 22:25, 25:27, 27:28\n",
    "                slice_idx += i\n",
    "        if self.log: \n",
    "            for seg in x_seg: print(seg)\n",
    "        if self.log: print('*'*30)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # red triangle stage\n",
    "            if i == 0:\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x_seg[i])\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "            elif i == 1:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len, red_tri_layer_num = x_cpy.shape[1], self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i > 1 and i < self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len = x_cpy.shape[1]\n",
    "                red_tri_layer_num = self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                # insert input segment to x_cpy\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                # jump connection, add prevprev layer output to this layer output\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i >= self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 0::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "        # select all the even index of x\n",
    "        x = x[:, 0::2]\n",
    "\n",
    "        return x\n",
    "            \n",
    "            \n",
    "\n",
    "model = Script_DiaNet(25, 10)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n",
    "\n",
    "x = torch.randn(1, 25)\n",
    "res = model(x)\n",
    "tar = torch.randn(1, 10)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "loss = mseloss(res, tar)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, device) -> None:\n",
    "        super(MainModel, self).__init__()\n",
    "        self.submodels = nn.ModuleList([Script_DiaNet(patch_size, patch_out, device) for _ in range(patch_num)])\n",
    "        self.main_module = Script_DiaNet(main_head,10,device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3, 'main, err input.shape: {}'.format(x.shape)\n",
    "        assert (x.shape[1]==patch_num)and(x.shape[2]==patch_size), 'main, err input.shape: {}'.format(x.shape)\n",
    "\n",
    "        sub_results = []\n",
    "        for i in range(patch_num):\n",
    "            sub_results.append(self.submodels[i](x[:,i,:]))\n",
    "        sub_results = torch.cat(sub_results, dim=1)\n",
    "        assert sub_results.shape[1] == main_head, 'main, err sub_results.shape: {}'.format(sub_results.shape)\n",
    "\n",
    "        output = self.main_module(sub_results)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.8351143092743114\n",
      "saved at epoch 0, acc 0.4913, loss 1.5643947622444057\n",
      "epoch 1, loss 1.4927214950895005\n",
      "saved at epoch 1, acc 0.6134, loss 1.4298165463194061\n",
      "epoch 2, loss 1.3887324981343772\n",
      "saved at epoch 2, acc 0.6222, loss 1.3443417790569836\n",
      "epoch 3, loss 1.326346592099936\n",
      "saved at epoch 3, acc 0.6389, loss 1.2957456353344494\n",
      "epoch 4, loss 1.2880477806144177\n",
      "saved at epoch 4, acc 0.6768, loss 1.2663712576974797\n",
      "epoch 5, loss 1.251227458656978\n",
      "saved at epoch 5, acc 0.687, loss 1.2224048330814024\n",
      "epoch 6, loss 1.2196066295668515\n",
      "saved at epoch 6, acc 0.7126, loss 1.195735077314739\n",
      "epoch 7, loss 1.1897476141386703\n",
      "saved at epoch 7, acc 0.7353, loss 1.1661207766472539\n",
      "epoch 8, loss 1.1637901743846153\n",
      "saved at epoch 8, acc 0.7422, loss 1.1448765857310235\n",
      "epoch 9, loss 1.1449389767799296\n",
      "saved at epoch 9, acc 0.7581, loss 1.1241626271718665\n",
      "epoch 10, loss 1.1296257277541577\n",
      "saved at epoch 10, acc 0.7606, loss 1.1121500089198728\n",
      "epoch 11, loss 1.1175513575071974\n",
      "saved at epoch 11, acc 0.7666, loss 1.105337991744657\n",
      "epoch 12, loss 1.1075353012410307\n",
      "saved at epoch 12, acc 0.7736, loss 1.0866079466252387\n",
      "epoch 13, loss 1.096887303313721\n",
      "saved at epoch 13, acc 0.7749, loss 1.0827334621284581\n",
      "epoch 14, loss 1.0883435214252106\n",
      "saved at epoch 14, acc 0.7773, loss 1.075500194030472\n",
      "epoch 15, loss 1.081879379398533\n",
      "saved at epoch 15, acc 0.7824, loss 1.0745065461231182\n",
      "epoch 16, loss 1.0728020357933126\n",
      "saved at epoch 16, acc 0.7899, loss 1.0615163950980464\n",
      "epoch 17, loss 1.0667876030590489\n",
      "saved at epoch 17, acc 0.7917, loss 1.0536185797256759\n",
      "epoch 18, loss 1.0622653700649611\n",
      "epoch 19, loss 1.0572524975611965\n",
      "saved at epoch 19, acc 0.798, loss 1.0461172506779055\n",
      "epoch 20, loss 1.0531079858096677\n",
      "epoch 21, loss 1.0475630162875536\n",
      "epoch 22, loss 1.0462722517788283\n",
      "saved at epoch 22, acc 0.7991, loss 1.038286550135552\n",
      "epoch 23, loss 1.042405618406308\n",
      "epoch 24, loss 1.0377020514341815\n",
      "saved at epoch 24, acc 0.801, loss 1.0301676943332334\n",
      "epoch 25, loss 1.0360559161538\n",
      "saved at epoch 25, acc 0.8011, loss 1.0318474973304361\n",
      "epoch 26, loss 1.0330500410818089\n",
      "saved at epoch 26, acc 0.8042, loss 1.0293245074115223\n",
      "epoch 27, loss 1.0292557720690647\n",
      "epoch 28, loss 1.0268667336465962\n",
      "saved at epoch 28, acc 0.8069, loss 1.0212519621547265\n",
      "epoch 29, loss 1.0247359402906666\n",
      "saved at epoch 29, acc 0.8072, loss 1.0200381045100055\n",
      "epoch 30, loss 1.0228244844021828\n",
      "saved at epoch 30, acc 0.8081, loss 1.0160015285769595\n",
      "epoch 31, loss 1.022468542874749\n",
      "epoch 32, loss 1.0184290063406614\n",
      "saved at epoch 32, acc 0.8091, loss 1.014921364904959\n",
      "epoch 33, loss 1.0163573414277929\n",
      "epoch 34, loss 1.0140934965249573\n",
      "saved at epoch 34, acc 0.8115, loss 1.0099052009703238\n",
      "epoch 35, loss 1.009705422275356\n",
      "epoch 36, loss 1.0103481884704215\n",
      "epoch 37, loss 1.0073252142365299\n",
      "saved at epoch 37, acc 0.8125, loss 1.0062009367761733\n",
      "epoch 38, loss 1.0059425918532332\n",
      "saved at epoch 38, acc 0.8152, loss 1.00377693281898\n",
      "epoch 39, loss 1.005427631869245\n",
      "epoch 40, loss 1.004186561112719\n",
      "saved at epoch 40, acc 0.8178, loss 0.9988166267358805\n",
      "epoch 41, loss 1.0022492643866712\n",
      "saved at epoch 41, acc 0.8244, loss 0.9966789121869244\n",
      "epoch 42, loss 0.9932462620074307\n",
      "saved at epoch 42, acc 0.8938, loss 0.9818979826154588\n",
      "epoch 43, loss 0.977452651651175\n",
      "saved at epoch 43, acc 0.8993, loss 0.972542357595661\n",
      "epoch 44, loss 0.9740601497164159\n",
      "saved at epoch 44, acc 0.9005, loss 0.9704885618596137\n",
      "epoch 45, loss 0.9711508011258742\n",
      "epoch 46, loss 0.9696984940500402\n",
      "saved at epoch 46, acc 0.901, loss 0.97126027300388\n",
      "epoch 47, loss 0.9693701963689028\n",
      "saved at epoch 47, acc 0.9021, loss 0.9670103675202478\n",
      "epoch 48, loss 0.9676938926233157\n",
      "epoch 49, loss 0.9639920283482273\n",
      "saved at epoch 49, acc 0.9024, loss 0.9681950219069855\n",
      "epoch 50, loss 0.9623471952196377\n",
      "epoch 51, loss 0.9620383121311538\n",
      "saved at epoch 51, acc 0.908, loss 0.9648891196975226\n",
      "epoch 52, loss 0.9619089317982639\n",
      "epoch 53, loss 0.9594052841922621\n",
      "saved at epoch 53, acc 0.9085, loss 0.9606031684935847\n",
      "epoch 54, loss 0.9586151917097665\n",
      "epoch 55, loss 0.9582338011595232\n",
      "epoch 56, loss 0.9581022371869605\n",
      "saved at epoch 56, acc 0.9096, loss 0.9588825853565072\n",
      "epoch 57, loss 0.9561616357709808\n",
      "saved at epoch 57, acc 0.9109, loss 0.9575866626787789\n",
      "epoch 58, loss 0.9553255248171434\n",
      "epoch 59, loss 0.9536290375916943\n",
      "epoch 60, loss 0.9545239803633456\n",
      "epoch 61, loss 0.9537512836679976\n",
      "saved at epoch 61, acc 0.9117, loss 0.9582038838652116\n",
      "epoch 62, loss 0.9523791417892553\n",
      "epoch 63, loss 0.9518982480838101\n",
      "epoch 64, loss 0.9525901578636821\n",
      "epoch 65, loss 0.948815272815192\n",
      "epoch 66, loss 0.9481952885574878\n",
      "saved at epoch 66, acc 0.9118, loss 0.9511107462870924\n",
      "epoch 67, loss 0.9473168476304011\n",
      "epoch 68, loss 0.9483745587405874\n",
      "epoch 69, loss 0.9471940390590919\n",
      "saved at epoch 69, acc 0.9124, loss 0.9491460164891014\n",
      "epoch 70, loss 0.9465880080072611\n",
      "epoch 71, loss 0.9457814820539723\n",
      "epoch 72, loss 0.9453651789409011\n",
      "saved at epoch 72, acc 0.9173, loss 0.9457376516318019\n",
      "epoch 73, loss 0.9443890542617993\n",
      "epoch 74, loss 0.9438740023926123\n",
      "epoch 75, loss 0.9444255084117085\n",
      "epoch 76, loss 0.9436230440892136\n",
      "epoch 77, loss 0.9419661094384916\n",
      "epoch 78, loss 0.9413540523443649\n",
      "epoch 79, loss 0.9402051540071776\n",
      "saved at epoch 79, acc 0.9179, loss 0.9446354659297799\n",
      "epoch 80, loss 0.9410936814635548\n",
      "saved at epoch 80, acc 0.9184, loss 0.9441584235505213\n",
      "epoch 81, loss 0.9402307401587968\n",
      "saved at epoch 81, acc 0.9194, loss 0.9420009300678591\n",
      "epoch 82, loss 0.939681394522124\n",
      "epoch 83, loss 0.9384234336647652\n",
      "epoch 84, loss 0.9403587716983072\n",
      "epoch 85, loss 0.9387829217321074\n",
      "epoch 86, loss 0.937189151483304\n",
      "saved at epoch 86, acc 0.92, loss 0.9393708532369589\n",
      "epoch 87, loss 0.9384028846775291\n",
      "epoch 88, loss 0.9372347322608362\n",
      "epoch 89, loss 0.93771752111439\n",
      "saved at epoch 89, acc 0.9225, loss 0.93673412256603\n",
      "epoch 90, loss 0.9362305850108296\n",
      "epoch 91, loss 0.9355651496061638\n",
      "epoch 92, loss 0.9349315954423917\n",
      "epoch 93, loss 0.9362645173377828\n",
      "epoch 94, loss 0.935135239858363\n",
      "epoch 95, loss 0.9339699579962789\n",
      "epoch 96, loss 0.9336825377905547\n",
      "epoch 97, loss 0.9328742900382735\n",
      "epoch 98, loss 0.9327659118912621\n",
      "epoch 99, loss 0.9318436914161324\n",
      "saved at epoch 99, acc 0.9238, loss 0.9351129328148274\n",
      "epoch 100, loss 0.9318676481623132\n",
      "epoch 101, loss 0.9311426941519861\n",
      "epoch 102, loss 0.9305706918875037\n",
      "epoch 103, loss 0.9325393129513462\n",
      "epoch 104, loss 0.9301668095436177\n",
      "saved at epoch 104, acc 0.9241, loss 0.9345260722727715\n",
      "epoch 105, loss 0.9300001398332591\n",
      "epoch 106, loss 0.9303052811734458\n",
      "epoch 107, loss 0.928451668479041\n",
      "epoch 108, loss 0.9300179925046241\n",
      "epoch 109, loss 0.9289364526266737\n",
      "epoch 110, loss 0.9295160321792814\n",
      "epoch 111, loss 0.929829717953322\n",
      "epoch 112, loss 0.9294161103935893\n",
      "epoch 113, loss 0.9273051877519978\n",
      "saved at epoch 113, acc 0.9244, loss 0.9315218201166466\n",
      "epoch 114, loss 0.9280533499555039\n",
      "epoch 115, loss 0.9275522960274458\n",
      "saved at epoch 115, acc 0.9251, loss 0.9307887795605237\n",
      "epoch 116, loss 0.9295475318996128\n",
      "epoch 117, loss 0.9284214202020722\n",
      "epoch 118, loss 0.9276290338939187\n",
      "epoch 119, loss 0.9278871184473099\n",
      "epoch 120, loss 0.9272833813482256\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "device = torch.device('cpu')\n",
    "model = MainModel(device).to(device)\n",
    "# model.load_state_dict(torch.load('saveddict/125_25_wider.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "model.train()\n",
    "max_acc, min_loss = 0.0, 100\n",
    "for epoch in range(5000):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, patch_num, patch_size)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print('epoch {}, loss {}'.format(epoch, epoch_loss))\n",
    "\n",
    "    # test model\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(-1, patch_num, patch_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        epoch_loss /= len(test_loader)\n",
    "        \n",
    "        # save best model, both accuracy and loss\n",
    "        if correct/total > max_acc: #and loss.item() < min_loss:\n",
    "            max_acc = correct/total\n",
    "            torch.save(model.state_dict(), 'saveddict/{}x{}_{}_wider.pth'.format(patch_num, patch_out, patch_size))\n",
    "            print('saved at epoch {}, acc {}, loss {}'.format(epoch, max_acc, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
