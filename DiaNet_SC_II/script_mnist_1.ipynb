{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 121, 25])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# load patches data from files\n",
    "# train_images_patches = np.load('data/train_crop25_L2_121_25.npy') # After L2 normalization\n",
    "# test_images_patches = np.load('data/test_crop25_L2_121_25.npy')\n",
    "train_images_patches = np.load('data/train_crop25_121_25.npy') # no L2\n",
    "test_images_patches = np.load('data/test_crop25_121_25.npy')\n",
    "train_images_patches = torch.from_numpy(train_images_patches)\n",
    "test_images_patches = torch.from_numpy(test_images_patches)\n",
    "\n",
    "# get the label from datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_offtrain = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_offtest = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_labels = [label for _, label in mnist_offtrain]\n",
    "test_labels = [label for _, label in mnist_offtest]\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# train_labels = torch.cat([train_labels, test_labels], dim=0)\n",
    "\n",
    "# make them to be PyTorch tensors, and dataloader\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_images_patches[:100], train_labels[:100])\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_images_patches[:100], test_labels[:100])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images_patches, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_images_patches, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# test dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_debug(header:str, debug_npy:np.array):\n",
    "    # convert numpy array to string\n",
    "    debug_str = ''\n",
    "    for i in range(debug_npy.shape[0]):\n",
    "        for j in range(debug_npy.shape[1]):\n",
    "            debug_str += str(debug_npy[i][j])[0:5] + ' '\n",
    "        debug_str += '\\n'\n",
    "        \n",
    "    # write debug_str to debug.txt\n",
    "    with open('debug/debug.txt', 'a') as f:\n",
    "        f.write(header+': \\t'+debug_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0652, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# clear debug.txt\n",
    "with open('debug/debug.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "class Script_DiaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_num, out_num, device='cpu'):\n",
    "        super(Script_DiaNet, self).__init__()\n",
    "        self.log = False\n",
    "        \n",
    "        self.red_depths = list(range(2, 1000))\n",
    "        self.red_fulls = [sum(range(1, n+2)) for n in self.red_depths]\n",
    "        self.red_heads = [(x * 2) + 1 for x in self.red_depths]\n",
    "\n",
    "        self.red_full = -1\n",
    "        self.red_dep = -1\n",
    "        self.red_triangle = []\n",
    "        self.layers, self.masks = self.gen_layers(inp_num, out_num)\n",
    "        # each mask to device\n",
    "        for i in range(len(self.masks)):\n",
    "            self.masks[i] = self.masks[i].to(device)\n",
    "        # each layers multiply mask\n",
    "        self.x_hidden = []\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].weight.data = self.layers[i].weight.data * self.masks[i]\n",
    "            self.x_hidden.append(-1) # init x_hidden\n",
    "        \n",
    "    \n",
    "    def select_insert(self, n, sel_len) -> list:\n",
    "        assert (((n - 1) // 2) % 2) == (sel_len % 2), \"Invalid selection length\"\n",
    "        numbers = range(n)\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        center_index = len(odd_numbers) // 2\n",
    "        selected_numbers = odd_numbers[center_index - sel_len // 2: center_index + sel_len // 2 + (sel_len % 2)]\n",
    "        return selected_numbers if sel_len != 1 else selected_numbers\n",
    "        \n",
    "    def gen_layers(self, inp_num, out_num):\n",
    "        # create Red triangle(input triangle)\n",
    "        masks_list = []\n",
    "        self.bn_list = [] # batch norm list\n",
    "\n",
    "        # select input triangle by input num\n",
    "        red_dep, red_head = -1, -1\n",
    "        for i in range(len(self.red_fulls)-1):\n",
    "            if inp_num > self.red_fulls[i] and inp_num <= self.red_fulls[i+1]:\n",
    "                red_full = self.red_fulls[i+1]\n",
    "                red_dep = self.red_depths[i+1]\n",
    "                red_head = self.red_heads[i+1]\n",
    "                if self.log: print(\"red_dep:\", red_dep, \"red_head:\", red_head)\n",
    "                break\n",
    "        assert (red_dep>0)and(red_head>0), \"red_dep {} or head {} is not valid\".format(red_dep, red_head)\n",
    "        self.red_dep = red_dep\n",
    "        self.red_full = red_full\n",
    "        for i in range(red_dep, 0, -1):\n",
    "            if i == red_dep:\n",
    "                self.red_triangle.append(i*2+1)\n",
    "            else:\n",
    "                self.red_triangle.append(i)\n",
    "\n",
    "        \n",
    "        # create blue triangle(output triangle)\n",
    "        nn_output = out_num + (out_num-1)\n",
    "        blue_dep = int(nn_output/2-0.5)\n",
    "        if self.log:print(\"blue_dep:\", blue_dep)\n",
    "\n",
    "        total_dep = red_dep + blue_dep\n",
    "        if self.log:print(\"total_dep:\", total_dep)\n",
    "\n",
    "        # expand times\n",
    "        expand_times = total_dep - red_dep\n",
    "        if self.log:print(\"expand_times:\", expand_times)\n",
    "\n",
    "        # create expand time nn layer\n",
    "        expand_list = []\n",
    "        expand_list.append(nn.Linear(red_head, red_head-2, bias=False)) # first layer\n",
    "        self.bn_list.append(nn.BatchNorm1d(red_head-2))\n",
    "        masks_list.append(self.fst_mask(red_head, red_head-2))\n",
    "        nn_in = red_head-2\n",
    "        for i in range(expand_times):\n",
    "            expand_list.append(nn.Linear(nn_in, nn_in+2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in+2))\n",
    "            masks_list.append(self.exp_mask_v2(nn_in, nn_in+2))\n",
    "            nn_in += 2\n",
    "        if self.log:print(\"expand_list:\", expand_list)\n",
    "\n",
    "        # shrink times\n",
    "        shrink_times = total_dep - blue_dep - 1\n",
    "        # if self.log:print(\"shrink_times:\", shrink_times)\n",
    "        shrink_list = []\n",
    "        for i in range(shrink_times):\n",
    "            shrink_list.append(nn.Linear(nn_in, nn_in-2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in-2))\n",
    "            masks_list.append(self.shr_mask_v2(nn_in, nn_in-2))\n",
    "            nn_in -= 2\n",
    "        if self.log:print(\"shrink_list:\", shrink_list)\n",
    "\n",
    "        # combine expand and shrink list\n",
    "        layer_list = expand_list + shrink_list\n",
    "        layers = nn.Sequential(*layer_list)\n",
    "        assert len(masks_list) == len(layer_list), \"masks_list {} and layer_list {} is not same\".format(len(masks_list), len(layer_list))\n",
    "\n",
    "        return layers, masks_list\n",
    "    \n",
    "    def fst_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"fst_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                start_idx = i\n",
    "                end_idx = i+2 + 1\n",
    "                mask[i, start_idx:end_idx] = 1\n",
    "        return mask\n",
    "    \n",
    "    def exp_mask(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-4)\n",
    "                b = min(max(0, i-2), in_dim-1)\n",
    "                c = min(max(0, i-1), in_dim-1)\n",
    "                d = min(max(0, i), in_dim-1)\n",
    "                e = min(max(0, i+2), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "    \n",
    "    def shr_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-2)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def exp_mask_v2(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-6)\n",
    "                b = min(max(0, i-4), in_dim-1)\n",
    "                c = min(max(0, i-2), in_dim-1)\n",
    "                d = min(max(0, i-1), in_dim-1)\n",
    "                e = min(max(0, i), in_dim-1)\n",
    "                f = min(max(0, i+2), in_dim-1)\n",
    "                g = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e,f,g]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def shr_mask_v2(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a_ = max(0, i-4)\n",
    "                a = min(max(0, i-2), in_dim-1)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                e_ = min(max(0, i+6), in_dim-1)\n",
    "                idx_lis = [a_,a,b,c,d,e,e_]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "                \n",
    "    def forward(self, x):\n",
    "        container = torch.zeros((x.shape[0], self.red_full)) # (b, 28)\n",
    "        container[:, 0:x.shape[1]] = x\n",
    "        # split x for red triangle, froexample, 25-> 13,5,4,3,2,1\n",
    "        x_seg = []\n",
    "        slice_idx = 0\n",
    "        for i in range(self.red_dep, 0, -1):\n",
    "            if i == self.red_dep:\n",
    "                x_seg.append(container[:, 0:i*2+1]) # 13\n",
    "                slice_idx += i*2+1\n",
    "            else:\n",
    "                x_seg.append(container[:, slice_idx:slice_idx+i]) # 13:18, 18:22, 22:25, 25:27, 27:28\n",
    "                slice_idx += i\n",
    "        if self.log: \n",
    "            for seg in x_seg: print(seg)\n",
    "        if self.log: print('*'*30)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # red triangle stage\n",
    "            if i == 0:\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x_seg[i])\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "            elif i == 1:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len, red_tri_layer_num = x_cpy.shape[1], self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i > 1 and i < self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len = x_cpy.shape[1]\n",
    "                red_tri_layer_num = self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                # insert input segment to x_cpy\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                # jump connection, add prevprev layer output to this layer output\n",
    "                # FIXME: add this.\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i >= self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 0::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "        # select all the even index of x\n",
    "        x = x[:, 0::2]\n",
    "\n",
    "        return x\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "model = Script_DiaNet(25, 10)\n",
    "x = torch.randn(64, 25)\n",
    "res = model(x)\n",
    "tar = torch.randn(64, 10)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "loss = mseloss(res, tar)\n",
    "print(loss)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, device) -> None:\n",
    "        super(MainModel, self).__init__()\n",
    "        self.submodels = nn.ModuleList([Script_DiaNet(25,1,device) for _ in range(121)])\n",
    "        self.main_module = Script_DiaNet(121,10,device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3, 'main, err input.shape: {}'.format(x.shape)\n",
    "        assert (x.shape[1]==121)and(x.shape[2]==25), 'main, err input.shape: {}'.format(x.shape)\n",
    "\n",
    "        sub_results = []\n",
    "        for i in range(121):\n",
    "            sub_results.append(self.submodels[i](x[:,i,:]))\n",
    "        sub_results = torch.cat(sub_results, dim=1)\n",
    "        assert sub_results.shape[1] == 121, 'main, err sub_results.shape: {}'.format(sub_results.shape)\n",
    "\n",
    "        output = self.main_module(sub_results)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.555421364586999\n",
      "saved at epoch 0, acc 0.8149, loss 1.2448628416544274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     22\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 23\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     24\u001b[0m epoch_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, epoch_loss))\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:351\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    354\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "device = torch.device('cpu')\n",
    "model = MainModel(device).to(device)\n",
    "# model.load_state_dict(torch.load('saveddict/121_25.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "model.train()\n",
    "max_acc, min_loss = 0.0, 100\n",
    "for epoch in range(500):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, 121, 25)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print('epoch {}, loss {}'.format(epoch, epoch_loss))\n",
    "\n",
    "    # test model\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(-1, 121, 25)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        epoch_loss /= len(test_loader)\n",
    "        \n",
    "        # save best model, both accuracy and loss\n",
    "        if correct/total > max_acc: #and loss.item() < min_loss:\n",
    "            max_acc = correct/total\n",
    "            torch.save(model.state_dict(), 'saveddict/121_25_wider.pth')\n",
    "            print('saved at epoch {}, acc {}, loss {}'.format(epoch, max_acc, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "model = MainModel(device).to(device)\n",
    "model.load_state_dict(torch.load('saveddict/121_25.pth'))\n",
    "\n",
    "# test_images and test_dataset have same order\n",
    "test_loader2 = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "# output a res, check the output\n",
    "wrong_idx = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_loader2):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = images.view(-1, 121, 25)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # append wrong idx\n",
    "        if predicted != labels:\n",
    "            wrong_idx.append(i)\n",
    "\n",
    "print(len(wrong_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use wrong idx to get the wrong images from test_images\n",
    "wrong_images = test_images[wrong_idx]\n",
    "# and plot first 400 of them in subplot(20,20)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(373):\n",
    "    ax = fig.add_subplot(20, 20, i+1)\n",
    "    ax.imshow(wrong_images[i].reshape(25,25), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
