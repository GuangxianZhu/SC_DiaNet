{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 49, 25])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# load patches data from files\n",
    "train_images_patches = torch.load('data/mnist77_train.pt')\n",
    "test_images_patches = torch.load('data/mnist77_test.pt')\n",
    "\n",
    "# get the label from datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "test_labels = [label for _, label in test_dataset]\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# make them to be PyTorch tensors, and dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images_patches, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_images_patches, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.9920806288719177\n",
      "saved at epoch 0, acc 0.8828, loss 0.9270277619361877\n",
      "epoch 1, loss 0.8643274903297424\n",
      "saved at epoch 1, acc 0.9374, loss 0.8525916337966919\n",
      "epoch 2, loss 0.9249877333641052\n",
      "saved at epoch 2, acc 0.9455, loss 0.8390929102897644\n",
      "epoch 3, loss 0.9426184296607971\n",
      "saved at epoch 3, acc 0.9489, loss 0.8282407522201538\n",
      "epoch 4, loss 0.9606946706771851\n",
      "saved at epoch 4, acc 0.9531, loss 0.8311588168144226\n",
      "epoch 5, loss 0.899727463722229\n",
      "saved at epoch 5, acc 0.9552, loss 0.8234381079673767\n",
      "epoch 6, loss 0.9742513298988342\n",
      "saved at epoch 6, acc 0.9565, loss 0.8133790493011475\n",
      "epoch 7, loss 0.8697620630264282\n",
      "saved at epoch 7, acc 0.9578, loss 0.8142099976539612\n",
      "epoch 8, loss 0.912955641746521\n",
      "saved at epoch 8, acc 0.9607, loss 0.8099356889724731\n",
      "epoch 9, loss 0.8643361926078796\n",
      "saved at epoch 9, acc 0.962, loss 0.8086739182472229\n",
      "epoch 10, loss 0.915657639503479\n",
      "saved at epoch 10, acc 0.963, loss 0.8061193823814392\n",
      "epoch 11, loss 0.8203417062759399\n",
      "saved at epoch 11, acc 0.9654, loss 0.8071283102035522\n",
      "epoch 12, loss 0.9513874053955078\n",
      "epoch 13, loss 0.9193122982978821\n",
      "saved at epoch 13, acc 0.9669, loss 0.805040180683136\n",
      "epoch 14, loss 0.8849226236343384\n",
      "saved at epoch 14, acc 0.9681, loss 0.802815854549408\n",
      "epoch 15, loss 0.9396699666976929\n",
      "saved at epoch 15, acc 0.9685, loss 0.8030304312705994\n",
      "epoch 16, loss 0.9433889985084534\n",
      "epoch 17, loss 0.8465200662612915\n",
      "saved at epoch 17, acc 0.9704, loss 0.803499698638916\n",
      "epoch 18, loss 0.8821021318435669\n",
      "epoch 19, loss 0.8151238560676575\n",
      "epoch 20, loss 0.8939300775527954\n",
      "epoch 21, loss 0.8962641954421997\n",
      "epoch 22, loss 0.8529565334320068\n",
      "saved at epoch 22, acc 0.9705, loss 0.8010907769203186\n",
      "epoch 23, loss 0.8898154497146606\n",
      "epoch 24, loss 0.8090351819992065\n",
      "epoch 25, loss 0.9094192385673523\n",
      "saved at epoch 25, acc 0.9706, loss 0.8013375401496887\n",
      "epoch 26, loss 0.8540387153625488\n",
      "epoch 27, loss 0.8123328685760498\n",
      "epoch 28, loss 0.8169335722923279\n",
      "saved at epoch 28, acc 0.9719, loss 0.8020687103271484\n",
      "epoch 29, loss 0.8544840216636658\n",
      "epoch 30, loss 0.8663550615310669\n",
      "saved at epoch 30, acc 0.9721, loss 0.8012562990188599\n",
      "epoch 31, loss 0.848063051700592\n",
      "saved at epoch 31, acc 0.9724, loss 0.8016916513442993\n",
      "epoch 32, loss 0.8093959093093872\n",
      "epoch 33, loss 0.9298330545425415\n",
      "epoch 34, loss 0.8352599740028381\n",
      "epoch 35, loss 0.8282670378684998\n",
      "epoch 36, loss 0.8164403438568115\n",
      "saved at epoch 36, acc 0.9735, loss 0.8003858923912048\n",
      "epoch 37, loss 0.9062464237213135\n",
      "saved at epoch 37, acc 0.975, loss 0.8014135956764221\n",
      "epoch 38, loss 0.8098698854446411\n",
      "epoch 39, loss 0.8422558903694153\n",
      "epoch 40, loss 0.8284221887588501\n",
      "epoch 41, loss 0.8243213295936584\n",
      "epoch 42, loss 0.829892635345459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 183\u001b[0m\n\u001b[0;32m    181\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    182\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 183\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    184\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m    186\u001b[0m \u001b[39m# test model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define a model\n",
    "# define sub model first\n",
    "class SubModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubModel, self).__init__()\n",
    "        # self.fc0 = nn.Linear(13, 11, bias=False)\n",
    "        # self.fc1 = nn.Linear(11, 9, bias=False)\n",
    "        # self.fc2 = nn.Linear(9, 7, bias=False)\n",
    "        # self.fc3 = nn.Linear(7, 5, bias=False)\n",
    "        # self.fc4 = nn.Linear(5, 3, bias=False)\n",
    "        # self.fc5 = nn.Linear(3, 1, bias=False)\n",
    "        # # masks\n",
    "        # self.mask0 = self.generate_mask(13, 11)\n",
    "        # self.mask1 = self.generate_mask(11, 9)\n",
    "        # self.mask2 = self.generate_mask(9, 7)\n",
    "        # self.mask3 = self.generate_mask(7, 5)\n",
    "        # self.mask4 = self.generate_mask(5, 3)\n",
    "        # self.mask5 = self.generate_mask(3, 1)\n",
    "\n",
    "        self.FC0 = nn.Linear(25, 10, bias=False)\n",
    "        self.FC1 = nn.Linear(10, 1, bias=False)\n",
    "\n",
    "        self.logg = False\n",
    "    \n",
    "    def generate_mask(self, in_dim, out_dim):\n",
    "        assert (in_dim % 2 == 1) and (out_dim % 2 == 1) and (in_dim-out_dim==2), \"in_dim and out_dim must be odd and in_dim-out_dim=2\"\n",
    "        mask = torch.zeros(out_dim, in_dim)\n",
    "        half_input_dim = in_dim // 2\n",
    "        for i in range(out_dim):\n",
    "            if i % 2 == 0:  # For every second row\n",
    "                start_index = i  # Determine start index for the 1's in this row\n",
    "                mask[i, start_index:start_index+half_input_dim] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == 25, \"x.shape[1] must be 25, but got {}\".format(x.shape[1])\n",
    "        assert len(x.shape) == 2, \"len(x.shape) must be 2, but got {}\".format(len(x.shape))\n",
    "\n",
    "        # self.fc0.weight.data *= self.mask0\n",
    "        # x0 = self.fc0(x[:, 0:13]) # x0(b,11)\n",
    "        # # insert x[:, 13:18] to x0[1], x0[3], x0[5], x0[7], x0[9]\n",
    "        # x0c = torch.cat((x0[:, 0:1], x[:, 13:14], x0[:, 2:3], x[:, 14:15], x0[:, 4:5], x[:, 15:16], \n",
    "        #                  x0[:, 6:7], x[:, 16:17], x0[:, 8:9], x[:, 17:18], x0[:, 10:11]), dim=1)\n",
    "        # x0c = torch.tanh(x0c)\n",
    "\n",
    "        # self.fc1.weight.data *= self.mask1\n",
    "        # x1 = self.fc1(x0c)\n",
    "        # # insert x[:, 18:22] to x1[1], x1[3], x1[5], x1[7]\n",
    "        # x1c = torch.cat((x1[:, 0:1], x[:, 18:19], x1[:, 2:3], x[:, 19:20], \n",
    "        #                  x1[:, 4:5], x[:, 20:21], x1[:, 6:7], x[:, 21:22], x1[:, 8:9]), dim=1)\n",
    "        # x1c = torch.tanh(x1c)\n",
    "\n",
    "        # self.fc2.weight.data *= self.mask2\n",
    "        # x2 = self.fc2(x1c)\n",
    "        # # insert x[:, 22:25] to x2[1], x2[3], x2[5]\n",
    "        # x2c = torch.cat((x2[:, 0:1], x[:, 22:23], x2[:, 2:3], x[:, 23:24], x2[:, 4:5], x[:, 24:25], x2[:, 6:7]), dim=1)\n",
    "        # x2c = torch.tanh(x2c)\n",
    "\n",
    "        # self.fc3.weight.data *= self.mask3\n",
    "        # x3 = self.fc3(x2c)\n",
    "        # # no insert from now on(fc3)\n",
    "        # x3 = torch.tanh(x3)\n",
    "\n",
    "        # self.fc4.weight.data *= self.mask4\n",
    "        # x4 = self.fc4(x3)\n",
    "        # x4 = torch.tanh(x4)\n",
    "\n",
    "        # self.fc5.weight.data *= self.mask5\n",
    "        # x5 = self.fc5(x4)\n",
    "        # x5 = torch.tanh(x5)\n",
    "\n",
    "\n",
    "        # if self.logg: print(x0, x0c, '\\n', x1, x1c, '\\n',\n",
    "        #                     x2, x2c, '\\n', x3, '\\n', x4, '\\n', x5)\n",
    "\n",
    "        self.FC0.weight.data = torch.clamp(self.FC0.weight.data, -1, 1)\n",
    "        x = self.FC0(x)\n",
    "        x = torch.tanh(x)\n",
    "        self.FC1.weight.data = torch.clamp(self.FC1.weight.data, -1, 1)\n",
    "        x = self.FC1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# test model use randn input\n",
    "# model = SubModel()\n",
    "# x = torch.randn(1, 25)\n",
    "# model(x)\n",
    "\n",
    "# define main model\n",
    "# main model contain 25 sub models, and concat the 25 submodels' output to a 25-dim vector\n",
    "# then use mainmodel(25,10) to process the 25-dim vector\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MainModel, self).__init__()\n",
    "        self.submodels = nn.ModuleList([SubModel() for _ in range(49)])\n",
    "        self.logg = False\n",
    "\n",
    "        # self.fc0 = nn.Linear(23, 21, bias=False)\n",
    "        # self.fc1 = nn.Linear(21, 19, bias=False)\n",
    "        # self.fc2 = nn.Linear(19, 17, bias=False)\n",
    "        # self.fc3 = nn.Linear(17, 15, bias=False)\n",
    "        # self.fc4 = nn.Linear(15, 13, bias=False)\n",
    "        # self.fc5 = nn.Linear(13, 11, bias=False)\n",
    "\n",
    "        self.fcfc1 = nn.Linear(49, 16, bias=False)\n",
    "        self.fcfc2 = nn.Linear(16, 10, bias=False)\n",
    "\n",
    "        # self.mask0 = self.generate_mask(23, 21)\n",
    "        # self.mask1 = self.generate_mask(21, 19)\n",
    "\n",
    "    def generate_mask(self, in_dim, out_dim):\n",
    "        assert (in_dim % 2 == 1) and (out_dim % 2 == 1) and (in_dim-out_dim==2), \"in_dim and out_dim must be odd and in_dim-out_dim=2\"\n",
    "        mask = torch.zeros(out_dim, in_dim)\n",
    "        half_input_dim = in_dim // 2\n",
    "        for i in range(out_dim):\n",
    "            if i % 2 == 0:  # For every second row\n",
    "                start_index = i  # Determine start index for the 1's in this row\n",
    "                mask[i, start_index:start_index+half_input_dim] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, xx):\n",
    "        assert len(xx.shape) == 3, \"Main. len(x.shape) must be 3, but got {}\".format(len(xx.shape))\n",
    "        assert (xx.shape[1]==49) and (xx.shape[2]==25), \"Main. x.shape[1] and x.shape[2] must be 25, but got {} and {}\".format(xx.shape[1], xx.shape[2])\n",
    "        # the 25 input to 25 submodels\n",
    "        sub_results = []\n",
    "        for i in range(49):\n",
    "            sub_results.append(self.submodels[i](xx[:, i, :]))\n",
    "        \n",
    "        sub_results = torch.cat(sub_results, dim=1)\n",
    "        if self.logg: print('sub_results:',sub_results.shape)\n",
    "        assert sub_results.shape[1] == 49, \"sub_results.shape[1] must be 49, but got {}\".format(sub_results.shape[1])\n",
    "\n",
    "        # directly use fcfc to process the 25-dim vector\n",
    "        self.fcfc1.weight.data = torch.clamp(self.fcfc1.weight.data, -1, 1)\n",
    "        x1c = self.fcfc1(sub_results)\n",
    "        x1c = torch.tanh(x1c)\n",
    "        self.fcfc2.weight.data = torch.clamp(self.fcfc2.weight.data, -1, 1)\n",
    "        x2 = self.fcfc2(x1c)\n",
    "        x2c = torch.tanh(x2)\n",
    "\n",
    "        # # process the 25-dim vector\n",
    "        # # self.fc0.weight.data *= self.mask0\n",
    "        # x0 = self.fc0(sub_results[:, 0:23])\n",
    "        # x0c = torch.cat((x0[:, 0:9], sub_results[:, 23:24], x0[:, 10:11], sub_results[:, 24:25], x0[:, 12:21]), dim=1)\n",
    "        # x0c = torch.tanh(x0c)\n",
    "\n",
    "        # # self.fc1.weight.data *= self.mask1\n",
    "        # x1 = self.fc1(x0c)\n",
    "        # x1 = torch.tanh(x1) # (b, 19)\n",
    "\n",
    "        # # select the 10-dim vector from the 19-dim vector\n",
    "        # # select 0, 2, 4, 6, 8, 10, 12, 14, 16, 18\n",
    "        # x1c = torch.cat((x1[:, 0:1], x1[:, 2:3], x1[:, 4:5], x1[:, 6:7], x1[:, 8:9],\n",
    "        #                     x1[:, 10:11], x1[:, 12:13], x1[:, 14:15], x1[:, 16:17], x1[:, 18:19]), dim=1)\n",
    "        # x1c = torch.tanh(x1c)\n",
    "\n",
    "\n",
    "        return x2c\n",
    "\n",
    "# use randn input to test main model\n",
    "# model = MainModel()\n",
    "# x = torch.randn(1, 25, 25)\n",
    "# model(x) # (1, 10)\n",
    "\n",
    "# train the model ********************************************************\n",
    "model = MainModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "max_acc, min_loss = 0, 100\n",
    "for epoch in range(70):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, 49, 25)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "    # test model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 49, 25)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # save best model, both accuracy and loss\n",
    "        if correct/total > max_acc: #and loss.item() < min_loss:\n",
    "            max_acc = correct/total\n",
    "            min_loss = loss.item()\n",
    "            #torch.save(model.state_dict(), 'data/patch_mnist.pth')\n",
    "            print('saved at epoch {}, acc {}, loss {}'.format(epoch, max_acc, min_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
