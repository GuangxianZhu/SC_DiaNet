{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyper parameters for MNIST\n",
    "inputSize = 784\n",
    "hiddenLayerSize = 100\n",
    "numberClasses = 10\n",
    "learningRate = 0.001\n",
    "\n",
    "# Load MNIST dataset\n",
    "trainDataset = torchvision.datasets.MNIST('./data', train = True, transform = transforms.ToTensor(), download = True)\n",
    "testDataset = torchvision.datasets.MNIST('./data', train = False, transform = transforms.ToTensor())\n",
    "trainLoader = torch.utils.data.DataLoader(dataset = trainDataset, batch_size=24, shuffle=False)\n",
    "testLoader = torch.utils.data.DataLoader(dataset = testDataset, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction_3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        logg = False\n",
    "        # The forward pass can use ctx.\n",
    "        #ctx.save_for_backward(input, weight, bias)\n",
    "        count = torch.zeros_like(weight)\n",
    "        for k in range(4):\n",
    "            referenceParam = torch.rand_like(weight)\n",
    "            if logg: print(\"fowd.ge\", torch.ge(weight, referenceParam))\n",
    "            count = torch.add( torch.ge(weight, referenceParam).float(), count)\n",
    "            if logg: print(\"fowd.count\", count)\n",
    "        tempWeight = torch.ge(count, 2).float()\n",
    "        if logg: print(\"fowd.tempWeight\", tempWeight)\n",
    "        output = input.mm(tempWeight.t())\n",
    "        if bias is not None:\n",
    "            count = torch.zeros_like(bias)\n",
    "            for k in range(4):\n",
    "                referenceParam = torch.rand_like(bias)\n",
    "                count = torch.add( torch.ge(bias, referenceParam).float(), count)\n",
    "            tempBias = torch.ge(count, 2).float()\n",
    "            if logg: print(\"fowd.tempBias\", tempBias)\n",
    "            output += tempBias.unsqueeze(0).expand_as(output)\n",
    "        ctx.save_for_backward(input, tempWeight, tempBias)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        logg = False\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        grad_output = F.hardtanh(grad_output)\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "            # if logg: print(\"back.grad_input\", grad_input)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            if logg: print(\"back.grad_weight\", grad_weight)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "            \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight before BP tensor([ 0.6662,  0.5546,  1.5303,  1.7600, -0.8034, -0.3979, -0.6043, -0.4749,\n",
      "        -1.5421,  0.4470, -2.0763,  2.0132,  0.5412, -0.8705, -2.0640,  1.5530,\n",
      "        -0.2717,  0.2575, -0.2516, -0.2860], grad_fn=<SelectBackward0>)\n",
      "back.grad_weight tensor([[ 3.8006e-02,  3.8738e-02,  1.3425e-02,  1.8539e-02, -2.0649e-02,\n",
      "         -3.8879e-02,  1.1634e-02,  1.8019e-02, -2.4792e-02,  5.9863e-02,\n",
      "         -3.5627e-02,  1.8798e-02,  1.4933e-02, -3.8263e-02,  1.1714e-02,\n",
      "         -1.9371e-02,  8.8255e-04,  1.6703e-02,  7.1645e-03, -8.2428e-03],\n",
      "        [-2.3888e-02, -2.0530e-02,  2.1598e-02, -4.5838e-02,  5.6860e-03,\n",
      "          2.4632e-02,  1.5597e-02,  5.1192e-03,  5.5282e-02,  6.5036e-02,\n",
      "         -3.0847e-02, -1.6248e-03,  5.6512e-02,  4.9141e-02, -2.6338e-02,\n",
      "         -7.1078e-02,  5.6660e-02,  1.7842e-02,  1.6961e-02,  9.3998e-02],\n",
      "        [ 5.5395e-02,  6.9032e-02,  4.8156e-02, -1.4181e-02, -2.0249e-03,\n",
      "         -3.4189e-02,  6.3744e-03,  5.0665e-02, -3.7175e-02,  7.6627e-02,\n",
      "         -2.8902e-02,  2.4975e-02, -1.6259e-02, -3.4446e-02, -1.5732e-02,\n",
      "         -2.6772e-02,  1.6586e-02,  6.8444e-02,  6.7871e-02,  5.1568e-03],\n",
      "        [ 2.0141e-02,  4.9139e-02,  1.4163e-03, -2.6205e-02,  3.1559e-02,\n",
      "         -2.7065e-02,  4.4339e-02, -2.5588e-02, -3.1217e-02,  3.5718e-03,\n",
      "          3.5371e-02, -2.8763e-02, -5.5764e-03,  2.3564e-03, -2.5817e-02,\n",
      "          3.3634e-02,  2.1277e-02,  1.1337e-01,  2.7897e-02,  4.5233e-02],\n",
      "        [-2.5460e-02,  1.3667e-02,  2.2637e-02, -6.4259e-02,  1.3132e-01,\n",
      "          4.0890e-02, -4.9659e-02, -3.4640e-02, -3.8290e-03, -4.4572e-02,\n",
      "          4.9691e-02,  3.3632e-02, -3.8159e-02,  3.6160e-02, -5.1129e-02,\n",
      "          6.0247e-02, -5.3831e-02,  3.5782e-02,  5.9487e-02,  4.4409e-02],\n",
      "        [-5.9997e-03,  3.1398e-02,  2.8802e-02, -2.5137e-02,  2.9164e-02,\n",
      "          1.4781e-02,  5.2662e-02,  2.2416e-03, -3.3394e-03,  1.1468e-01,\n",
      "          4.1256e-02, -5.2660e-02,  6.0391e-02, -9.4723e-03, -5.4109e-04,\n",
      "         -8.1633e-02,  7.8588e-02,  9.9910e-02,  6.7469e-03,  6.1303e-02],\n",
      "        [ 1.3149e-02, -9.1293e-03,  4.0031e-02, -1.1345e-02,  3.6786e-02,\n",
      "         -1.9732e-03, -1.9668e-02,  1.2632e-02, -4.8566e-02,  1.2477e-02,\n",
      "          6.4004e-02,  2.3528e-02, -4.3145e-02, -2.5757e-02,  2.5504e-03,\n",
      "          9.3534e-03,  1.3877e-03,  5.9659e-02,  6.4518e-02,  8.3980e-02],\n",
      "        [-6.2951e-02, -5.5776e-02,  2.2325e-02, -5.0750e-02,  4.9676e-02,\n",
      "          1.1222e-02, -1.2095e-02,  1.4803e-02,  4.7155e-02,  1.0919e-01,\n",
      "          4.1170e-02,  3.4970e-02,  3.5556e-02,  7.0406e-02, -8.6722e-04,\n",
      "         -4.4462e-02,  4.3580e-02,  2.3366e-02, -7.7655e-03, -1.6981e-02],\n",
      "        [ 6.9453e-02,  4.7098e-02, -1.0153e-04,  3.8506e-03,  1.7538e-02,\n",
      "          2.6012e-02, -4.7840e-02,  1.3012e-02, -3.5183e-02, -5.2014e-02,\n",
      "         -2.3060e-02,  3.9987e-02, -4.0203e-02,  2.7739e-03, -2.6027e-02,\n",
      "          1.7235e-03, -5.3379e-02,  1.2812e-02,  4.8513e-02,  4.8627e-02],\n",
      "        [ 2.7334e-02,  2.8940e-02,  3.0727e-03,  1.4664e-02, -1.9221e-02,\n",
      "         -3.1083e-02,  7.6095e-03, -1.0256e-02, -2.3405e-02,  2.0129e-02,\n",
      "         -5.7929e-03,  1.8285e-02,  1.7169e-02,  1.9424e-02,  2.9578e-04,\n",
      "         -6.7247e-03,  5.5635e-02,  1.0763e-01,  2.7879e-02,  5.5728e-04],\n",
      "        [-4.8523e-02, -6.9890e-03,  3.1050e-03, -7.9427e-02,  9.6595e-02,\n",
      "          5.0580e-02,  2.8516e-02, -9.6469e-02,  7.8624e-02,  1.0347e-01,\n",
      "          7.1106e-02, -4.6968e-02,  3.5053e-02,  1.1489e-01, -8.9495e-02,\n",
      "         -5.5559e-05,  1.0034e-01,  2.0466e-01,  1.8236e-02,  7.6203e-02],\n",
      "        [ 3.3024e-02,  6.0422e-02,  3.1872e-02,  4.4180e-02, -4.6230e-02,\n",
      "         -2.0553e-02,  4.3742e-02,  4.5342e-02, -2.1611e-02,  1.0549e-01,\n",
      "          2.9096e-02, -4.7332e-02,  4.2849e-02, -6.2335e-02,  7.2641e-02,\n",
      "         -1.0962e-01,  8.1898e-02,  6.6751e-02, -1.8417e-02, -5.4721e-02],\n",
      "        [ 4.8414e-02,  7.2816e-02,  2.9773e-02,  6.2616e-03, -2.4848e-02,\n",
      "         -1.3801e-02,  1.5914e-02,  1.3838e-02,  3.8612e-02, -1.9714e-02,\n",
      "         -3.5929e-02, -2.3352e-02, -9.4321e-03, -2.1485e-02,  2.5676e-02,\n",
      "         -4.4783e-02, -5.3347e-03, -1.7577e-03,  1.5679e-02, -1.1355e-03],\n",
      "        [ 2.5670e-02,  2.5794e-02,  5.0572e-02, -5.5038e-02,  4.6115e-02,\n",
      "         -2.9920e-02,  3.1746e-02, -1.7837e-02,  4.0807e-02,  6.3661e-02,\n",
      "         -3.2381e-02, -1.0454e-02, -2.1861e-02, -4.5912e-03, -6.2964e-02,\n",
      "          2.0296e-02, -2.7293e-02,  5.3778e-02,  7.5690e-02,  1.4311e-01],\n",
      "        [ 1.3296e-02, -1.7260e-02, -1.5319e-03,  1.7703e-02, -1.2153e-02,\n",
      "          8.8002e-03,  1.2499e-02, -5.1824e-03,  2.2905e-02,  9.0686e-02,\n",
      "          7.3477e-03, -5.2371e-03,  1.4395e-02,  9.9231e-03,  3.9830e-03,\n",
      "         -4.4552e-02,  3.5102e-02,  4.1847e-02, -2.2954e-02, -1.5215e-02],\n",
      "        [ 4.4515e-03,  1.1891e-01,  3.9359e-02, -1.9148e-02,  6.0525e-02,\n",
      "         -8.3991e-03,  4.1973e-03,  1.0978e-02, -4.0491e-02, -1.8611e-02,\n",
      "         -3.5897e-02,  3.3898e-03,  2.2716e-02, -6.8220e-03, -4.5957e-03,\n",
      "         -4.8056e-02,  3.1493e-02,  5.8983e-02,  6.5183e-02,  3.6423e-02],\n",
      "        [ 1.9420e-02, -2.6336e-03, -7.6378e-03, -3.2676e-02,  5.0449e-02,\n",
      "          1.3395e-02, -7.8539e-03, -4.8971e-02,  4.9343e-02, -7.3780e-03,\n",
      "          6.5228e-02,  7.8966e-04, -3.3658e-03,  1.0076e-01, -3.1746e-02,\n",
      "          1.7025e-02,  1.1890e-02,  1.4769e-01,  3.0752e-02,  2.0360e-02],\n",
      "        [-2.7486e-03, -8.7205e-04,  2.6927e-02,  1.4047e-02, -2.3015e-02,\n",
      "          8.8975e-03,  5.7547e-03,  3.8643e-02, -3.9007e-03,  3.9407e-03,\n",
      "          1.5592e-03, -4.8721e-03,  4.6341e-03,  1.2692e-02,  4.2704e-02,\n",
      "         -6.4610e-02,  3.3443e-02, -1.5898e-02, -1.5946e-02, -2.3868e-02],\n",
      "        [-4.5591e-02,  3.7586e-02,  3.3622e-02, -7.7043e-02,  7.4801e-02,\n",
      "         -1.8098e-02,  1.6694e-02,  5.5901e-03,  4.4428e-02,  7.3365e-02,\n",
      "         -3.3360e-02,  2.3211e-02,  7.2372e-02,  8.1595e-02, -2.2807e-02,\n",
      "         -6.6616e-02,  4.0449e-02,  4.4059e-02,  3.0499e-02,  8.0218e-02],\n",
      "        [-5.9579e-02, -7.5662e-03, -7.3293e-04, -3.8702e-02,  7.7781e-02,\n",
      "         -7.8097e-04,  2.5497e-03, -3.4995e-02,  2.8642e-02, -9.6735e-03,\n",
      "          5.9567e-02, -4.9552e-03,  3.2222e-02,  1.3194e-02,  7.1888e-03,\n",
      "          3.0604e-02,  1.6465e-03,  2.9157e-02,  8.1488e-03, -2.0460e-02],\n",
      "        [ 9.2715e-03, -2.7346e-02,  5.5966e-02, -7.1027e-02,  3.9534e-02,\n",
      "          1.8845e-02,  2.4850e-02,  1.6615e-02,  6.8694e-02,  1.2281e-01,\n",
      "          2.3865e-02, -2.1353e-02,  6.6310e-03,  4.7850e-02, -5.8737e-02,\n",
      "         -3.2414e-02,  5.6920e-03,  7.5007e-02,  4.5703e-02,  4.3501e-02],\n",
      "        [-3.4199e-02,  8.4487e-03,  3.8166e-02,  2.8016e-02, -3.1511e-02,\n",
      "          4.0061e-02,  3.6610e-03,  4.5658e-02, -5.4316e-03,  5.0173e-02,\n",
      "          5.8526e-03, -1.7651e-02,  2.2634e-02, -6.5869e-02,  7.6822e-02,\n",
      "         -1.3462e-01,  1.0150e-01, -5.1887e-02, -1.6185e-02,  2.1652e-02],\n",
      "        [-3.5926e-02, -2.7504e-02, -1.9811e-02, -3.3255e-02,  5.2074e-02,\n",
      "         -2.2994e-02, -3.8506e-02, -1.4934e-02,  1.1444e-02, -2.4233e-02,\n",
      "         -9.0961e-04,  7.1647e-02, -5.5567e-03,  1.2242e-01, -2.8273e-02,\n",
      "          4.8576e-02,  6.2324e-03,  5.4864e-02,  1.8262e-02,  7.1407e-03],\n",
      "        [-1.5214e-02,  3.2433e-02,  3.9499e-03,  6.4226e-04,  1.1106e-01,\n",
      "         -2.7647e-02, -6.9843e-02,  6.7055e-03, -1.2260e-01, -7.3810e-02,\n",
      "          4.2591e-02,  1.0318e-01, -5.8781e-02,  1.0152e-02,  1.9515e-02,\n",
      "          4.6176e-02, -3.1394e-02,  3.3175e-02,  6.6284e-02,  1.1080e-01],\n",
      "        [-4.7463e-03, -2.4902e-02, -3.1198e-03, -6.5506e-03, -1.9891e-02,\n",
      "         -1.9430e-02, -1.2603e-02,  2.1093e-02,  2.8083e-02,  7.0245e-03,\n",
      "         -1.4355e-02,  2.9738e-02, -4.9550e-03,  4.7864e-03,  1.0617e-02,\n",
      "          1.3390e-02, -1.0294e-02, -3.5220e-02, -1.7075e-03, -3.5660e-02],\n",
      "        [-7.6385e-02,  5.3208e-02,  4.8070e-02, -7.1498e-02,  1.1587e-01,\n",
      "         -1.9535e-02, -2.2728e-02,  7.4882e-03, -6.7105e-02, -1.9246e-02,\n",
      "          2.3125e-02,  7.7231e-02,  7.2614e-03,  6.4727e-02,  5.6206e-03,\n",
      "         -1.7267e-02,  5.4110e-02,  4.9576e-02,  5.0587e-02,  5.6062e-02],\n",
      "        [-5.7219e-02,  2.6616e-02,  2.9463e-02, -4.3025e-02, -2.1614e-03,\n",
      "         -2.7155e-02,  6.4744e-02, -1.1423e-02,  3.4401e-02,  3.6847e-02,\n",
      "         -1.6768e-02, -2.9694e-02,  5.9765e-02,  7.8241e-02,  6.6876e-03,\n",
      "         -5.1253e-02,  1.0652e-01,  6.1543e-02, -2.6990e-02, -3.0483e-02],\n",
      "        [ 2.5631e-03,  3.7512e-04,  1.4925e-02, -1.7559e-02,  9.7425e-04,\n",
      "          4.4365e-02,  1.0330e-02,  4.1668e-03,  1.0451e-02,  6.0476e-02,\n",
      "          9.3448e-03, -2.2710e-02,  4.6233e-02,  4.0287e-03, -1.6790e-02,\n",
      "         -7.1451e-02,  5.8198e-02,  5.0450e-02,  1.6702e-02,  5.8465e-02],\n",
      "        [ 4.5686e-03,  2.2356e-02, -1.6689e-02, -3.5502e-02,  4.1051e-02,\n",
      "         -2.6683e-02,  4.7273e-02, -3.6820e-02, -1.6099e-03, -5.5141e-03,\n",
      "         -1.1092e-03, -1.7950e-02,  3.3113e-02,  1.5623e-02, -3.4249e-02,\n",
      "          5.6033e-02, -2.3083e-02,  5.5351e-02, -1.6440e-03, -7.6689e-03],\n",
      "        [ 6.3128e-02,  3.4390e-02, -1.1323e-02,  5.1528e-03, -1.7183e-02,\n",
      "          5.7916e-02,  1.7029e-02, -4.1694e-02,  6.7451e-02,  1.5456e-02,\n",
      "          4.5484e-02, -7.9193e-02, -4.4867e-03,  9.4808e-03, -2.8379e-02,\n",
      "         -2.4281e-02,  2.8011e-02,  1.1518e-01,  3.4997e-03, -1.3034e-02]])\n",
      "weight after BP tensor([ 0.6282,  0.5158,  1.5169,  1.7414, -0.7828, -0.3591, -0.6159, -0.4929,\n",
      "        -1.5173,  0.3872, -2.0407,  1.9944,  0.5262, -0.8323, -2.0757,  1.5724,\n",
      "        -0.2726,  0.2408, -0.2587, -0.2778], grad_fn=<SelectBackward0>)\n",
      "loss tensor(5.9516, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class TestLinear(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TestLinear, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(output_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return LinearFunction_3.apply(input, self.weight, self.bias)\n",
    "\n",
    "# Use double precision as gradcheck requires it to create accurate numerical gradients\n",
    "x = torch.randn(20, 20, requires_grad=True)\n",
    "model = TestLinear(20, 30)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# print weight before BP\n",
    "print(\"weight before BP\", model.weight[0])\n",
    "res = model(x)\n",
    "target = torch.randn(30)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "loss = loss_func(res, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "# print weight after BP\n",
    "print(\"weight after BP\", model.weight[0])\n",
    "print(\"loss\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear_3(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here\n",
    "        return LinearFunction_3.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL _ 2 _ 24\n",
    "# Unipolar\n",
    "# bitwidth = 1\n",
    "# Voting Mechansim = 4\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenLayersSize, numberClasses):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_1 = CustomLinear_3(inputSize, hiddenLayersSize, bias = True)\n",
    "        self.linear_2 = CustomLinear_3(hiddenLayersSize, numberClasses, bias = True)\n",
    "        \n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenLayersSize = hiddenLayersSize\n",
    "        self.numberClasses = numberClasses\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear_1(x)\n",
    "        hid = output\n",
    "        output = torch.clamp(output, 0, 1)\n",
    "        activ = output\n",
    "        output = self.linear_2(output)\n",
    "        return output, hid, activ\n",
    "    \n",
    "model_2_24 = NeuralNetwork(inputSize, 500, numberClasses)\n",
    "model_2_24_losses = []\n",
    "model_2_24_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/300, step 2500 / 2500, loss = 2.2801\n",
      "hid tensor([[ 0.,  0.,  1.,  ...,  0.,  0.,  5.],\n",
      "        [ 1.,  0.,  0.,  ...,  3.,  0.,  1.],\n",
      "        [ 3.,  0.,  2.,  ...,  3.,  2., 11.],\n",
      "        ...,\n",
      "        [ 3.,  0.,  0.,  ...,  4.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [ 4.,  1.,  1.,  ...,  2.,  2., 13.]]) \n",
      "\n",
      "activ tensor([[0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]) \n",
      "\n",
      "accuracy = 78.26522435897436\n",
      "epoch 2/300, step 2500 / 2500, loss = 1.2963\n",
      "hid tensor([[0., 3., 2.,  ..., 0., 3., 1.],\n",
      "        [0., 0., 1.,  ..., 3., 0., 0.],\n",
      "        [2., 2., 2.,  ..., 1., 7., 9.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 4., 2., 0.],\n",
      "        [0., 0., 2.,  ..., 1., 1., 0.],\n",
      "        [2., 4., 2.,  ..., 1., 7., 9.]]) \n",
      "\n",
      "activ tensor([[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]) \n",
      "\n",
      "accuracy = 84.30488782051282\n",
      "epoch 3/300, step 2500 / 2500, loss = 1.0224\n",
      "hid tensor([[ 1.,  1.,  1.,  ...,  0.,  3.,  1.],\n",
      "        [ 2.,  0.,  0.,  ...,  2.,  0.,  0.],\n",
      "        [ 2.,  0.,  3.,  ...,  2.,  8.,  9.],\n",
      "        ...,\n",
      "        [ 4.,  0.,  1.,  ...,  4.,  2.,  0.],\n",
      "        [ 2.,  0.,  2.,  ...,  2.,  1.,  1.],\n",
      "        [ 4.,  1.,  3.,  ...,  1.,  8., 11.]]) \n",
      "\n",
      "activ tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]) \n",
      "\n",
      "accuracy = 85.08613782051282\n",
      "epoch 4/300, step 2500 / 2500, loss = 0.8832\n",
      "hid tensor([[ 2.,  0.,  1.,  ...,  0.,  3.,  3.],\n",
      "        [ 2.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        [ 2.,  0.,  3.,  ...,  0.,  6., 10.],\n",
      "        ...,\n",
      "        [ 2.,  0.,  0.,  ...,  3.,  2.,  0.],\n",
      "        [ 2.,  0.,  1.,  ...,  2.,  1.,  1.],\n",
      "        [ 3.,  0.,  2.,  ...,  0.,  6., 12.]]) \n",
      "\n",
      "activ tensor([[1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.]]) \n",
      "\n",
      "accuracy = 87.33974358974359\n",
      "epoch 5/300, step 2500 / 2500, loss = 0.7732\n",
      "hid tensor([[ 0.,  1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  2.,  ...,  0.,  5., 11.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1.,  ...,  3.,  1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  2.,  1.,  1.],\n",
      "        [ 1.,  1.,  2.,  ...,  0.,  6., 12.]]) \n",
      "\n",
      "activ tensor([[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.]]) \n",
      "\n",
      "accuracy = 86.39823717948718\n",
      "epoch 6/300, step 2500 / 2500, loss = 0.7195\n",
      "hid tensor([[ 0.,  0.,  0.,  ...,  0.,  3.,  3.],\n",
      "        [ 0.,  0.,  0.,  ...,  2.,  0.,  0.],\n",
      "        [ 1.,  0.,  1.,  ...,  0.,  6., 11.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  4.,  1.,  2.],\n",
      "        [ 0.,  0.,  0.,  ...,  2.,  0.,  2.],\n",
      "        [ 1.,  0.,  0.,  ...,  1.,  7., 14.]]) \n",
      "\n",
      "activ tensor([[0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.]]) \n",
      "\n",
      "accuracy = 90.1542467948718\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m samples \u001b[39m=\u001b[39m samples\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m784\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m outputs, _, _ \u001b[39m=\u001b[39m model_2_24(samples)\n\u001b[0;32m     30\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     31\u001b[0m totalLoss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_1(x)\n\u001b[0;32m     18\u001b[0m     hid \u001b[39m=\u001b[39m output\n\u001b[0;32m     19\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(output, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m, in \u001b[0;36mCustomLinear_3.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[39m# See the autograd section for explanation of what happens here\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m LinearFunction_3\u001b[39m.\u001b[39;49mapply(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mLinearFunction_3.forward\u001b[1;34m(ctx, input, weight, bias)\u001b[0m\n\u001b[0;32m      8\u001b[0m count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(weight)\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     referenceParam \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand_like(weight)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m logg: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfowd.ge\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mge(weight, referenceParam))\n\u001b[0;32m     12\u001b[0m     count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39madd( torch\u001b[39m.\u001b[39mge(weight, referenceParam)\u001b[39m.\u001b[39mfloat(), count)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(dataset = trainDataset, batch_size=24, shuffle=False, drop_last=True)\n",
    "testLoader = torch.utils.data.DataLoader(dataset = testDataset, batch_size=24, shuffle=False, drop_last=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_2_24.parameters(), lr = 7e-3)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(trainLoader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    totalLoss = 0\n",
    "    for i, (images, labels) in enumerate(trainLoader):\n",
    "        \n",
    "        # Change the input data format of the mnist dataset to stochastic form using reference random numbers\n",
    "        referenceTrainData = torch.rand(24, 1, 28, 28)/20\n",
    "        # samples = torch.zeros(24, 1, 28, 28)\n",
    "        \n",
    "        samples = torch.ge(images, referenceTrainData).float()\n",
    "        # plot the samples, then break\n",
    "        # plt.imshow(samples[i][0], cmap='gray')\n",
    "        # plt.show()\n",
    "                \n",
    "        samples = samples.reshape(-1, 784)\n",
    "             \n",
    "        # forward\n",
    "        outputs, _, _ = model_2_24(samples)\n",
    "        loss = criterion(outputs, labels)\n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if (i+1) % 2500 == 0:\n",
    "            model_2_24_losses.append(totalLoss/2500)\n",
    "            print(f'epoch {epoch+1}/{300}, step {i+1} / {n_total_steps}, loss = {totalLoss/2500:.4f}')\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        numberCorrect = 0\n",
    "        numberSamples = 0\n",
    "        for z, (images, labels) in enumerate(testLoader):  \n",
    "\n",
    "            # Change the input data format of the mnist dataset to stochastic form using reference random numbers\n",
    "            samples = torch.zeros(24, 1, 28, 28)\n",
    "            referenceTrainData = torch.rand(24, 1, 28, 28)/20\n",
    "            \n",
    "            samples = torch.ge(images, referenceTrainData).float()\n",
    "\n",
    "            images = samples.reshape(-1, 784)\n",
    "\n",
    "            outputs, hid, activ = model_2_24(images)\n",
    "\n",
    "            # value and index of the correct predictions\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            numberSamples += labels.shape[0]\n",
    "\n",
    "            numberCorrect += (predictions == labels).sum().item()\n",
    "\n",
    "            # if z == 415:\n",
    "            #     break\n",
    "\n",
    "        accuracy = 100.0 * (numberCorrect) / numberSamples\n",
    "        print(\"hid\", hid, '\\n')\n",
    "        print(\"activ\", activ, '\\n')\n",
    "\n",
    "        print(f'accuracy = {accuracy}')\n",
    "        model_2_24_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hid:  tensor([[0., 0., 0.,  ..., 0., 0., 2.],\n",
      "        [3., 0., 1.,  ..., 4., 1., 3.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 2., 0., 4.],\n",
      "        [3., 0., 0.,  ..., 0., 1., 4.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 3.]])\n",
      "activ:  tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.]]) \n",
      "\n",
      "outputs tensor([3., 4., 4., 4., 2., 2., 1., 3., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    numberCorrect = 0\n",
    "    numberSamples = 0\n",
    "    for z, (images, labels) in enumerate(testLoader):  \n",
    "\n",
    "        # Change the input data format of the mnist dataset to stochastic form using reference random numbers\n",
    "        samples = torch.zeros(24, 1, 28, 28)\n",
    "        referenceTrainData = torch.rand(24, 1, 28, 28)/20\n",
    "        \n",
    "        samples = torch.ge(images, referenceTrainData).float()\n",
    "\n",
    "        images = samples.reshape(-1, 784)\n",
    "\n",
    "        outputs = model_2_24(images)\n",
    "        print(\"outputs\", outputs[0])\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
