{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 144, 9])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# load patches data from files\n",
    "train_images_patches = np.load('data/train_12x12_3x3.npy')\n",
    "test_images_patches = np.load('data/test_12x12_3x3.npy')\n",
    "train_images_patches = torch.from_numpy(train_images_patches)\n",
    "test_images_patches = torch.from_numpy(test_images_patches)\n",
    "\n",
    "# get the label from datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_offtrain = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_offtest = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_labels = [label for _, label in mnist_offtrain]\n",
    "test_labels = [label for _, label in mnist_offtest]\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# make them to be PyTorch tensors, and dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images_patches, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_images_patches, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# test dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "\n",
    "patch_size = 3*3\n",
    "patch_num = 12*12\n",
    "patch_out = 1\n",
    "main_head = patch_num * patch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0514, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def write_debug(header:str, debug_npy:np.array):\n",
    "    # convert numpy array to string\n",
    "    debug_str = ''\n",
    "    for i in range(debug_npy.shape[0]):\n",
    "        for j in range(debug_npy.shape[1]):\n",
    "            debug_str += str(debug_npy[i][j])[0:5] + ' '\n",
    "        debug_str += '\\n'\n",
    "        \n",
    "    # write debug_str to debug.txt\n",
    "    with open('debug/debug.txt', 'a') as f:\n",
    "        f.write(header+': \\t'+debug_str + '\\n')\n",
    "        \n",
    "# clear debug.txt\n",
    "with open('debug/debug.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "class Script_DiaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_num, out_num, device='cpu'):\n",
    "        super(Script_DiaNet, self).__init__()\n",
    "        self.log = False\n",
    "        \n",
    "        self.red_depths = list(range(2, 1000))\n",
    "        self.red_fulls = [sum(range(1, n+2)) for n in self.red_depths]\n",
    "        self.red_heads = [(x * 2) + 1 for x in self.red_depths]\n",
    "\n",
    "        self.red_full = -1\n",
    "        self.red_dep = -1\n",
    "        self.red_triangle = []\n",
    "        self.jump_weights = nn.ParameterList()\n",
    "        self.layers, self.masks = self.gen_layers(inp_num, out_num)\n",
    "        # each mask to device\n",
    "        for i in range(len(self.masks)):\n",
    "            self.masks[i] = self.masks[i].to(device)\n",
    "        # each layers multiply mask\n",
    "        self.x_hidden = []\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].weight.data = self.layers[i].weight.data * self.masks[i]\n",
    "            self.x_hidden.append(-1) # init x_hidden\n",
    "        \n",
    "    \n",
    "    def select_insert(self, n, sel_len) -> list:\n",
    "        assert (((n - 1) // 2) % 2) == (sel_len % 2), \"Invalid selection length\"\n",
    "        numbers = range(n)\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        center_index = len(odd_numbers) // 2\n",
    "        selected_numbers = odd_numbers[center_index - sel_len // 2: center_index + sel_len // 2 + (sel_len % 2)]\n",
    "        return selected_numbers if sel_len != 1 else selected_numbers\n",
    "        \n",
    "    def gen_layers(self, inp_num, out_num):\n",
    "        # create Red triangle(input triangle)\n",
    "        masks_list = []\n",
    "        self.bn_list = [] # batch norm list\n",
    "\n",
    "        # select input triangle by input num\n",
    "        red_dep, red_head = -1, -1\n",
    "        for i in range(len(self.red_fulls)-1):\n",
    "            if inp_num > self.red_fulls[i] and inp_num <= self.red_fulls[i+1]:\n",
    "                red_full = self.red_fulls[i+1]\n",
    "                red_dep = self.red_depths[i+1]\n",
    "                red_head = self.red_heads[i+1]\n",
    "                if self.log: print(\"red_dep:\", red_dep, \"red_head:\", red_head)\n",
    "                break\n",
    "        assert (red_dep>0)and(red_head>0), \"red_dep {} or head {} is not valid\".format(red_dep, red_head)\n",
    "        self.red_dep = red_dep\n",
    "        self.red_full = red_full\n",
    "        for i in range(red_dep, 0, -1):\n",
    "            if i == red_dep:\n",
    "                self.red_triangle.append(i*2+1)\n",
    "            else:\n",
    "                self.red_triangle.append(i)\n",
    "\n",
    "        \n",
    "        # create blue triangle(output triangle)\n",
    "        nn_output = out_num + (out_num-1)\n",
    "        blue_dep = int(nn_output/2-0.5)\n",
    "        if self.log:print(\"blue_dep:\", blue_dep)\n",
    "\n",
    "        total_dep = red_dep + blue_dep\n",
    "        if self.log:print(\"total_dep:\", total_dep)\n",
    "\n",
    "        # expand times\n",
    "        expand_times = total_dep - red_dep\n",
    "        if self.log:print(\"expand_times:\", expand_times)\n",
    "\n",
    "        # create expand time nn layer\n",
    "        expand_list = []\n",
    "        expand_list.append(nn.Linear(red_head, red_head-2, bias=False)) # first layer\n",
    "        self.bn_list.append(nn.BatchNorm1d(red_head-2))\n",
    "        self.jump_weights.append(nn.Parameter(torch.randn(1, red_head-2))) # Not in use\n",
    "\n",
    "        masks_list.append(self.fst_mask(red_head, red_head-2))\n",
    "        nn_in = red_head-2\n",
    "        for i in range(expand_times):\n",
    "            expand_list.append(nn.Linear(nn_in, nn_in+2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in+2))\n",
    "            masks_list.append(self.exp_mask_v2(nn_in, nn_in+2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in += 2\n",
    "        if self.log:print(\"expand_list:\", expand_list)\n",
    "\n",
    "        # shrink times\n",
    "        shrink_times = total_dep - blue_dep - 1\n",
    "        # if self.log:print(\"shrink_times:\", shrink_times)\n",
    "        shrink_list = []\n",
    "        for i in range(shrink_times):\n",
    "            shrink_list.append(nn.Linear(nn_in, nn_in-2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in-2))\n",
    "            masks_list.append(self.shr_mask_v2(nn_in, nn_in-2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in -= 2\n",
    "        if self.log:print(\"shrink_list:\", shrink_list)\n",
    "\n",
    "        # combine expand and shrink list\n",
    "        layer_list = expand_list + shrink_list\n",
    "        layers = nn.Sequential(*layer_list)\n",
    "        assert len(masks_list) == len(layer_list), \"masks_list {} and layer_list {} is not same\".format(len(masks_list), len(layer_list))\n",
    "\n",
    "        return layers, masks_list\n",
    "    \n",
    "    def fst_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"fst_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                start_idx = i\n",
    "                end_idx = i+2 + 1\n",
    "                mask[i, start_idx:end_idx] = 1\n",
    "        return mask\n",
    "    \n",
    "    def exp_mask(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-4)\n",
    "                b = min(max(0, i-2), in_dim-1)\n",
    "                c = min(max(0, i-1), in_dim-1)\n",
    "                d = min(max(0, i), in_dim-1)\n",
    "                e = min(max(0, i+2), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "    \n",
    "    def shr_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-2)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def exp_mask_v2(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-6)\n",
    "                b = min(max(0, i-4), in_dim-1)\n",
    "                c = min(max(0, i-2), in_dim-1)\n",
    "                d = min(max(0, i-1), in_dim-1)\n",
    "                e = min(max(0, i), in_dim-1)\n",
    "                f = min(max(0, i+2), in_dim-1)\n",
    "                g = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e,f,g]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def shr_mask_v2(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a_ = max(0, i-4)\n",
    "                a = min(max(0, i-2), in_dim-1)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                e_ = min(max(0, i+6), in_dim-1)\n",
    "                idx_lis = [a_,a,b,c,d,e,e_]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        container = torch.zeros((x.shape[0], self.red_full)) # (b, 28)\n",
    "        container[:, 0:x.shape[1]] = x\n",
    "        # split x for red triangle, froexample, 25-> 13,5,4,3,2,1\n",
    "        x_seg = []\n",
    "        slice_idx = 0\n",
    "        for i in range(self.red_dep, 0, -1):\n",
    "            if i == self.red_dep:\n",
    "                x_seg.append(container[:, 0:i*2+1]) # 13\n",
    "                slice_idx += i*2+1\n",
    "            else:\n",
    "                x_seg.append(container[:, slice_idx:slice_idx+i]) # 13:18, 18:22, 22:25, 25:27, 27:28\n",
    "                slice_idx += i\n",
    "        if self.log: \n",
    "            for seg in x_seg: print(seg)\n",
    "        if self.log: print('*'*30)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # red triangle stage\n",
    "            if i == 0:\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x_seg[i])\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "            elif i == 1:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len, red_tri_layer_num = x_cpy.shape[1], self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i > 1 and i < self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len = x_cpy.shape[1]\n",
    "                red_tri_layer_num = self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                # insert input segment to x_cpy\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                # jump connection, add prevprev layer output to this layer output\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i >= self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 0::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "        # select all the even index of x\n",
    "        x = x[:, 0::2]\n",
    "\n",
    "        return x\n",
    "            \n",
    "            \n",
    "\n",
    "model = Script_DiaNet(25, 10)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n",
    "\n",
    "x = torch.randn(1, 25)\n",
    "res = model(x)\n",
    "tar = torch.randn(1, 10)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "loss = mseloss(res, tar)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, device) -> None:\n",
    "        super(MainModel, self).__init__()\n",
    "        self.submodels = nn.ModuleList([Script_DiaNet(patch_size, patch_out, device) for _ in range(patch_num)])\n",
    "        self.main_module = Script_DiaNet(main_head,10,device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3, 'main, err input.shape: {}'.format(x.shape)\n",
    "        assert (x.shape[1]==patch_num)and(x.shape[2]==patch_size), 'main, err input.shape: {}'.format(x.shape)\n",
    "\n",
    "        sub_results = []\n",
    "        for i in range(patch_num):\n",
    "            sub_results.append(self.submodels[i](x[:,i,:]))\n",
    "        sub_results = torch.cat(sub_results, dim=1)\n",
    "        assert sub_results.shape[1] == main_head, 'main, err sub_results.shape: {}'.format(sub_results.shape)\n",
    "\n",
    "        output = self.main_module(sub_results)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.6644011364816858\n",
      "saved at epoch 0, acc 0.6568, loss 1.379122421711306\n",
      "epoch 1, loss 1.315914503038565\n",
      "saved at epoch 1, acc 0.6886, loss 1.2565083201927474\n",
      "epoch 2, loss 1.2356589023492484\n",
      "saved at epoch 2, acc 0.7298, loss 1.2029158329661889\n",
      "epoch 3, loss 1.1942215824940565\n",
      "saved at epoch 3, acc 0.7345, loss 1.170479113542581\n",
      "epoch 4, loss 1.164957164955546\n",
      "saved at epoch 4, acc 0.7507, loss 1.1404156579247005\n",
      "epoch 5, loss 1.1447394692313189\n",
      "saved at epoch 5, acc 0.7531, loss 1.1284977242916445\n",
      "epoch 6, loss 1.1295797443593234\n",
      "saved at epoch 6, acc 0.7679, loss 1.1093556722508202\n",
      "epoch 7, loss 1.1146939720934643\n",
      "saved at epoch 7, acc 0.777, loss 1.0963901454889322\n",
      "epoch 8, loss 1.1046808345485597\n",
      "saved at epoch 8, acc 0.7798, loss 1.0862969763671295\n",
      "epoch 9, loss 1.0911330482852992\n",
      "saved at epoch 9, acc 0.7854, loss 1.0788055794148506\n",
      "epoch 10, loss 1.0824767000385438\n",
      "saved at epoch 10, acc 0.7925, loss 1.0640516047236286\n",
      "epoch 11, loss 1.0663199756445407\n",
      "saved at epoch 11, acc 0.8007, loss 1.0531643274464184\n",
      "epoch 12, loss 1.0541851559935858\n",
      "saved at epoch 12, acc 0.8094, loss 1.0368201189403292\n",
      "epoch 13, loss 1.0423228827112518\n",
      "saved at epoch 13, acc 0.8131, loss 1.0319901040837736\n",
      "epoch 14, loss 1.0324391336329202\n",
      "saved at epoch 14, acc 0.8185, loss 1.0212038264998906\n",
      "epoch 15, loss 1.0269487822996273\n",
      "saved at epoch 15, acc 0.8235, loss 1.0203752215904525\n",
      "epoch 16, loss 1.0231719948589675\n",
      "epoch 17, loss 1.0161035069778783\n",
      "saved at epoch 17, acc 0.8311, loss 1.013443475282645\n",
      "epoch 18, loss 1.0111455368334805\n",
      "saved at epoch 18, acc 0.8502, loss 1.0015773833552493\n",
      "epoch 19, loss 0.9963727064732549\n",
      "saved at epoch 19, acc 0.9006, loss 0.9819310488580149\n",
      "epoch 20, loss 0.9853106289784282\n",
      "saved at epoch 20, acc 0.9032, loss 0.9766216768494135\n",
      "epoch 21, loss 0.977635329720308\n",
      "saved at epoch 21, acc 0.9036, loss 0.9760521280614636\n",
      "epoch 22, loss 0.9746984257372712\n",
      "saved at epoch 22, acc 0.906, loss 0.9712858984741983\n",
      "epoch 23, loss 0.9682926468249323\n",
      "saved at epoch 23, acc 0.91, loss 0.9625111124183559\n",
      "epoch 24, loss 0.9667999203017017\n",
      "epoch 25, loss 0.963733061163156\n",
      "epoch 26, loss 0.9599483600303308\n",
      "epoch 27, loss 0.9565021923101787\n",
      "saved at epoch 27, acc 0.9167, loss 0.9516716041142428\n",
      "epoch 28, loss 0.9538004008183347\n",
      "epoch 29, loss 0.9514096963888546\n",
      "epoch 30, loss 0.9500808328199488\n",
      "saved at epoch 30, acc 0.9173, loss 0.9475208271907855\n",
      "epoch 31, loss 0.9475823311663386\n",
      "saved at epoch 31, acc 0.9174, loss 0.9472052817103229\n",
      "epoch 32, loss 0.945831370633294\n",
      "saved at epoch 32, acc 0.9184, loss 0.9458460702171808\n",
      "epoch 33, loss 0.9434227933253306\n",
      "saved at epoch 33, acc 0.9186, loss 0.9455460204353815\n",
      "epoch 34, loss 0.9408001331632325\n",
      "saved at epoch 34, acc 0.9199, loss 0.9439961653721484\n",
      "epoch 35, loss 0.9384261343016553\n",
      "epoch 36, loss 0.9391343043302931\n",
      "saved at epoch 36, acc 0.9244, loss 0.9377745812452292\n",
      "epoch 37, loss 0.9375103455362543\n",
      "epoch 38, loss 0.9345653135893441\n",
      "epoch 39, loss 0.9349422344266732\n",
      "saved at epoch 39, acc 0.9255, loss 0.9364240554314626\n",
      "epoch 40, loss 0.9337939539952065\n",
      "epoch 41, loss 0.9335551026787585\n",
      "epoch 42, loss 0.9304692775710051\n",
      "saved at epoch 42, acc 0.9266, loss 0.9309093620203719\n",
      "epoch 43, loss 0.9287838952373594\n",
      "epoch 44, loss 0.9281292459857997\n",
      "epoch 45, loss 0.9279952037817379\n",
      "saved at epoch 45, acc 0.9274, loss 0.9324064224581176\n",
      "epoch 46, loss 0.9266786314785354\n",
      "epoch 47, loss 0.9242966749520698\n",
      "epoch 48, loss 0.9233834878213878\n",
      "epoch 49, loss 0.9242224388285232\n",
      "saved at epoch 49, acc 0.9279, loss 0.9288133307348324\n",
      "epoch 50, loss 0.9224727156319852\n",
      "epoch 51, loss 0.9211939143727838\n",
      "epoch 52, loss 0.9207754396934753\n",
      "epoch 53, loss 0.9173347624634375\n",
      "saved at epoch 53, acc 0.9303, loss 0.9270044391668295\n",
      "epoch 54, loss 0.9185185701862327\n",
      "epoch 55, loss 0.9198412445308303\n",
      "epoch 56, loss 0.91836038988028\n",
      "saved at epoch 56, acc 0.9312, loss 0.9239026066623156\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "device = torch.device('cpu')\n",
    "model = MainModel(device).to(device)\n",
    "# model.load_state_dict(torch.load('saveddict/125_25_wider.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "model.train()\n",
    "max_acc, min_loss = 0.0, 100\n",
    "for epoch in range(5000):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, patch_num, patch_size)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print('epoch {}, loss {}'.format(epoch, epoch_loss))\n",
    "\n",
    "    # test model\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(-1, patch_num, patch_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        epoch_loss /= len(test_loader)\n",
    "        \n",
    "        # save best model, both accuracy and loss\n",
    "        if correct/total > max_acc: #and loss.item() < min_loss:\n",
    "            max_acc = correct/total\n",
    "            torch.save(model.state_dict(), 'saveddict/{}x{}_{}_wider.pth'.format(patch_num, patch_out, patch_size))\n",
    "            print('saved at epoch {}, acc {}, loss {}'.format(epoch, max_acc, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saveddict/144x1_9_wider.pth\n"
     ]
    }
   ],
   "source": [
    "print('saveddict/{}x{}_{}_wider.pth'.format(patch_num, patch_out, patch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
