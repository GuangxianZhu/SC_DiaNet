{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 121, 16])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# load patches data from files\n",
    "train_images_patches = np.load('data/train_11x11_4x4.npy')\n",
    "test_images_patches = np.load('data/test_11x11_4x4.npy')\n",
    "train_images_patches = torch.from_numpy(train_images_patches)\n",
    "test_images_patches = torch.from_numpy(test_images_patches)\n",
    "\n",
    "# get the label from datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_offtrain = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_offtest = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_labels = [label for _, label in mnist_offtrain]\n",
    "test_labels = [label for _, label in mnist_offtest]\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# make them to be PyTorch tensors, and dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images_patches, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_images_patches, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# test dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "\n",
    "patch_num = 11*11\n",
    "patch_size = 4*4\n",
    "patch_out = 1\n",
    "main_head = patch_num * patch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3697, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def write_debug(header:str, debug_npy:np.array):\n",
    "    # convert numpy array to string\n",
    "    debug_str = ''\n",
    "    for i in range(debug_npy.shape[0]):\n",
    "        for j in range(debug_npy.shape[1]):\n",
    "            debug_str += str(debug_npy[i][j])[0:5] + ' '\n",
    "        debug_str += '\\n'\n",
    "        \n",
    "    # write debug_str to debug.txt\n",
    "    with open('debug/debug.txt', 'a') as f:\n",
    "        f.write(header+': \\t'+debug_str + '\\n')\n",
    "        \n",
    "# clear debug.txt\n",
    "with open('debug/debug.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "class Script_DiaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_num, out_num, device='cpu'):\n",
    "        super(Script_DiaNet, self).__init__()\n",
    "        self.log = False\n",
    "        \n",
    "        self.red_depths = list(range(2, 1000))\n",
    "        self.red_fulls = [sum(range(1, n+2)) for n in self.red_depths]\n",
    "        self.red_heads = [(x * 2) + 1 for x in self.red_depths]\n",
    "\n",
    "        self.red_full = -1\n",
    "        self.red_dep = -1\n",
    "        self.red_triangle = []\n",
    "        self.jump_weights = nn.ParameterList()\n",
    "        self.layers, self.masks = self.gen_layers(inp_num, out_num)\n",
    "        # each mask to device\n",
    "        for i in range(len(self.masks)):\n",
    "            self.masks[i] = self.masks[i].to(device)\n",
    "        # each layers multiply mask\n",
    "        self.x_hidden = []\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].weight.data = self.layers[i].weight.data * self.masks[i]\n",
    "            self.x_hidden.append(-1) # init x_hidden\n",
    "        \n",
    "    \n",
    "    def select_insert(self, n, sel_len) -> list:\n",
    "        assert (((n - 1) // 2) % 2) == (sel_len % 2), \"Invalid selection length\"\n",
    "        numbers = range(n)\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        center_index = len(odd_numbers) // 2\n",
    "        selected_numbers = odd_numbers[center_index - sel_len // 2: center_index + sel_len // 2 + (sel_len % 2)]\n",
    "        return selected_numbers if sel_len != 1 else selected_numbers\n",
    "        \n",
    "    def gen_layers(self, inp_num, out_num):\n",
    "        # create Red triangle(input triangle)\n",
    "        masks_list = []\n",
    "        self.bn_list = [] # batch norm list\n",
    "\n",
    "        # select input triangle by input num\n",
    "        red_dep, red_head = -1, -1\n",
    "        for i in range(len(self.red_fulls)-1):\n",
    "            if inp_num > self.red_fulls[i] and inp_num <= self.red_fulls[i+1]:\n",
    "                red_full = self.red_fulls[i+1]\n",
    "                red_dep = self.red_depths[i+1]\n",
    "                red_head = self.red_heads[i+1]\n",
    "                if self.log: print(\"red_dep:\", red_dep, \"red_head:\", red_head)\n",
    "                break\n",
    "        assert (red_dep>0)and(red_head>0), \"red_dep {} or head {} is not valid\".format(red_dep, red_head)\n",
    "        self.red_dep = red_dep\n",
    "        self.red_full = red_full\n",
    "        for i in range(red_dep, 0, -1):\n",
    "            if i == red_dep:\n",
    "                self.red_triangle.append(i*2+1)\n",
    "            else:\n",
    "                self.red_triangle.append(i)\n",
    "\n",
    "        \n",
    "        # create blue triangle(output triangle)\n",
    "        nn_output = out_num + (out_num-1)\n",
    "        blue_dep = int(nn_output/2-0.5)\n",
    "        if self.log:print(\"blue_dep:\", blue_dep)\n",
    "\n",
    "        total_dep = red_dep + blue_dep\n",
    "        if self.log:print(\"total_dep:\", total_dep)\n",
    "\n",
    "        # expand times\n",
    "        expand_times = total_dep - red_dep\n",
    "        if self.log:print(\"expand_times:\", expand_times)\n",
    "\n",
    "        # create expand time nn layer\n",
    "        expand_list = []\n",
    "        expand_list.append(nn.Linear(red_head, red_head-2, bias=False)) # first layer\n",
    "        self.bn_list.append(nn.BatchNorm1d(red_head-2))\n",
    "        self.jump_weights.append(nn.Parameter(torch.randn(1, red_head-2))) # Not in use\n",
    "\n",
    "        masks_list.append(self.fst_mask(red_head, red_head-2))\n",
    "        nn_in = red_head-2\n",
    "        for i in range(expand_times):\n",
    "            expand_list.append(nn.Linear(nn_in, nn_in+2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in+2))\n",
    "            masks_list.append(self.exp_mask_v2(nn_in, nn_in+2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in += 2\n",
    "        if self.log:print(\"expand_list:\", expand_list)\n",
    "\n",
    "        # shrink times\n",
    "        shrink_times = total_dep - blue_dep - 1\n",
    "        # if self.log:print(\"shrink_times:\", shrink_times)\n",
    "        shrink_list = []\n",
    "        for i in range(shrink_times):\n",
    "            shrink_list.append(nn.Linear(nn_in, nn_in-2, bias=False))\n",
    "            self.bn_list.append(nn.BatchNorm1d(nn_in-2))\n",
    "            masks_list.append(self.shr_mask_v2(nn_in, nn_in-2))\n",
    "            self.jump_weights.append(nn.Parameter(torch.randn(1, nn_in-2)))\n",
    "            nn_in -= 2\n",
    "        if self.log:print(\"shrink_list:\", shrink_list)\n",
    "\n",
    "        # combine expand and shrink list\n",
    "        layer_list = expand_list + shrink_list\n",
    "        layers = nn.Sequential(*layer_list)\n",
    "        assert len(masks_list) == len(layer_list), \"masks_list {} and layer_list {} is not same\".format(len(masks_list), len(layer_list))\n",
    "\n",
    "        return layers, masks_list\n",
    "    \n",
    "    def fst_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"fst_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                start_idx = i\n",
    "                end_idx = i+2 + 1\n",
    "                mask[i, start_idx:end_idx] = 1\n",
    "        return mask\n",
    "    \n",
    "    def exp_mask(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-4)\n",
    "                b = min(max(0, i-2), in_dim-1)\n",
    "                c = min(max(0, i-1), in_dim-1)\n",
    "                d = min(max(0, i), in_dim-1)\n",
    "                e = min(max(0, i+2), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "    \n",
    "    def shr_mask(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-2)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def exp_mask_v2(self, in_dim, out_dim):\n",
    "        assert out_dim-in_dim == 2, \"exp_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a = max(0, i-6)\n",
    "                b = min(max(0, i-4), in_dim-1)\n",
    "                c = min(max(0, i-2), in_dim-1)\n",
    "                d = min(max(0, i-1), in_dim-1)\n",
    "                e = min(max(0, i), in_dim-1)\n",
    "                f = min(max(0, i+2), in_dim-1)\n",
    "                g = min(max(0, i+4), in_dim-1)\n",
    "                idx_lis = [a,b,c,d,e,f,g]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "\n",
    "    def shr_mask_v2(self, in_dim, out_dim):\n",
    "        assert in_dim-out_dim == 2, \"shr_mask: in_dim {} and out_dim {} is not valid\".format(in_dim, out_dim)\n",
    "        mask = torch.zeros((out_dim, in_dim))\n",
    "        for i in range(out_dim):\n",
    "            if i%2 == 0:\n",
    "                a_ = max(0, i-4)\n",
    "                a = min(max(0, i-2), in_dim-1)\n",
    "                b = min(max(0, i), in_dim-1)\n",
    "                c = min(max(0, i+1), in_dim-1)\n",
    "                d = min(max(0, i+2), in_dim-1)\n",
    "                e = min(max(0, i+4), in_dim-1)\n",
    "                e_ = min(max(0, i+6), in_dim-1)\n",
    "                idx_lis = [a_,a,b,c,d,e,e_]\n",
    "                mask[i, idx_lis] = 1\n",
    "        return mask\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        container = torch.zeros((x.shape[0], self.red_full)) # (b, 28)\n",
    "        container[:, 0:x.shape[1]] = x\n",
    "        # split x for red triangle, froexample, 25-> 13,5,4,3,2,1\n",
    "        x_seg = []\n",
    "        slice_idx = 0\n",
    "        for i in range(self.red_dep, 0, -1):\n",
    "            if i == self.red_dep:\n",
    "                x_seg.append(container[:, 0:i*2+1]) # 13\n",
    "                slice_idx += i*2+1\n",
    "            else:\n",
    "                x_seg.append(container[:, slice_idx:slice_idx+i]) # 13:18, 18:22, 22:25, 25:27, 27:28\n",
    "                slice_idx += i\n",
    "        if self.log: \n",
    "            for seg in x_seg: print(seg)\n",
    "        if self.log: print('*'*30)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # red triangle stage\n",
    "            if i == 0:\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x_seg[i])\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "            elif i == 1:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len, red_tri_layer_num = x_cpy.shape[1], self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i > 1 and i < self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 1::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x_cpy = x.clone()\n",
    "                this_len = x_cpy.shape[1]\n",
    "                red_tri_layer_num = self.red_triangle[i]\n",
    "                sel_insert = self.select_insert(this_len, red_tri_layer_num)\n",
    "                # if self.log: print(\"sel_insert:\", sel_insert)\n",
    "                assert x_cpy[:, sel_insert].shape == x_seg[i].shape, \"x_cpy[:, sel_insert] shape {} and x_seg[i] shape {} is not same\".format(x_cpy[:, sel_insert].shape, x_seg[i].shape)\n",
    "                if self.log: write_debug('layer{}, seg'.format(i), x_seg[i].detach().numpy())\n",
    "                # insert input segment to x_cpy\n",
    "                x_cpy[:, sel_insert] += x_seg[i]\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, x insrt'.format(i), x.detach().numpy())\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                # jump connection, add prevprev layer output to this layer output\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "\n",
    "            elif i >= self.red_dep:\n",
    "                if self.log: write_debug('layer{}, x'.format(i), x.detach().numpy())\n",
    "                assert torch.sum(x[:, 1::2]) == 0, \"x[:, 0::2] is not 0\"\n",
    "                layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "                layer.weight.data *= self.masks[i]\n",
    "                x = layer(x)\n",
    "                if self.log: write_debug('layer{}, x fc'.format(i), x.detach().numpy())\n",
    "                jump = self.x_hidden[i-2]\n",
    "                if self.log: write_debug('layer{}, jump'.format(i), jump.detach().numpy())\n",
    "                assert torch.sum(jump[:, 1::2]) == 0, \"jump[:, 1::2] is not 0\"\n",
    "                assert (jump.shape[1] == x.shape[1]+4) or (jump.shape[1] == x.shape[1]-4) or (jump.shape[1] == x.shape[1]), \"jump.shape[1] {} and x.shape[1] {} is not valid\".format(jump.shape[1], x.shape[1])\n",
    "                x_cpy = x.clone()\n",
    "                # compare x_cpy and jump\n",
    "                # jump_w = self.jump_weights[i]\n",
    "                # jump_w = torch.clamp(jump_w, min=-1, max=1)\n",
    "                if x_cpy.shape[1] > jump.shape[1]:\n",
    "                    x_cpy[:, 2:-2] += jump #* jump_w\n",
    "                elif x_cpy.shape[1] == jump.shape[1]:\n",
    "                    x_cpy += jump #* jump_w\n",
    "                else:\n",
    "                    x_cpy += jump[:, 2:-2] #* jump_w\n",
    "                x = x_cpy\n",
    "                if self.log: write_debug('layer{}, jump_w'.format(i), self.jump_weights[i].detach().numpy())\n",
    "                if self.log: write_debug('layer{}, x jump'.format(i), x.detach().numpy())\n",
    "                x = torch.tanh(x)\n",
    "                # x = self.bn_list[i](x)\n",
    "                self.x_hidden[i] = x\n",
    "            \n",
    "        # select all the even index of x\n",
    "        x = x[:, 0::2]\n",
    "\n",
    "        return x\n",
    "            \n",
    "            \n",
    "\n",
    "model = Script_DiaNet(25, 10)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n",
    "\n",
    "x = torch.randn(1, 25)\n",
    "res = model(x)\n",
    "tar = torch.randn(1, 10)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "loss = mseloss(res, tar)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# jump_w = model.jump_weights\n",
    "# for i in range(len(jump_w)):\n",
    "#     print(i, jump_w[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, device) -> None:\n",
    "        super(MainModel, self).__init__()\n",
    "        self.submodels = nn.ModuleList([Script_DiaNet(patch_size, patch_out, device) for _ in range(patch_num)])\n",
    "        self.main_module = Script_DiaNet(main_head,10,device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3, 'main, err input.shape: {}'.format(x.shape)\n",
    "        assert (x.shape[1]==patch_num)and(x.shape[2]==patch_size), 'main, err input.shape: {}'.format(x.shape)\n",
    "\n",
    "        sub_results = []\n",
    "        for i in range(patch_num):\n",
    "            sub_results.append(self.submodels[i](x[:,i,:]))\n",
    "        sub_results = torch.cat(sub_results, dim=1)\n",
    "        assert sub_results.shape[1] == main_head, 'main, err sub_results.shape: {}'.format(sub_results.shape)\n",
    "\n",
    "        output = self.main_module(sub_results)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.5698529662353906\n",
      "saved at epoch 0, acc 0.7329, loss 1.2734111864355546\n",
      "epoch 1, loss 1.2153271388397542\n",
      "saved at epoch 1, acc 0.7699, loss 1.166903387142133\n",
      "epoch 2, loss 1.140056641625443\n",
      "saved at epoch 2, acc 0.7828, loss 1.116042512881605\n",
      "epoch 3, loss 1.1041598759734554\n",
      "saved at epoch 3, acc 0.8002, loss 1.0860472359234774\n",
      "epoch 4, loss 1.084669820408323\n",
      "saved at epoch 4, acc 0.812, loss 1.0700211773944805\n",
      "epoch 5, loss 1.0688212127573709\n",
      "epoch 6, loss 1.0579144950868733\n",
      "epoch 7, loss 1.048428328433779\n",
      "epoch 8, loss 1.0428278448739285\n",
      "saved at epoch 8, acc 0.8157, loss 1.0396968737433228\n",
      "epoch 9, loss 1.0332895894802965\n",
      "saved at epoch 9, acc 0.8197, loss 1.0329157657261137\n",
      "epoch 10, loss 1.0252073821482628\n",
      "saved at epoch 10, acc 0.8221, loss 1.0243128855017167\n",
      "epoch 11, loss 1.0190897400953622\n",
      "epoch 12, loss 1.0123754652070085\n",
      "saved at epoch 12, acc 0.8239, loss 1.0139869994755033\n",
      "epoch 13, loss 1.0065537747035402\n",
      "epoch 14, loss 1.0000304058670744\n",
      "saved at epoch 14, acc 0.8336, loss 0.9979591445077823\n",
      "epoch 15, loss 0.9892690807009048\n",
      "epoch 16, loss 0.9855135838105988\n",
      "epoch 17, loss 0.9782853171006957\n",
      "saved at epoch 17, acc 0.8356, loss 0.9781808211833616\n",
      "epoch 18, loss 0.9751308424386389\n",
      "epoch 19, loss 0.9702671759926689\n",
      "saved at epoch 19, acc 0.8415, loss 0.9712325093112414\n",
      "epoch 20, loss 0.9661332709448678\n",
      "epoch 21, loss 0.9608953226603933\n",
      "saved at epoch 21, acc 0.8437, loss 0.9658242863944814\n",
      "epoch 22, loss 0.9581941893614178\n",
      "epoch 23, loss 0.9554374493769745\n",
      "saved at epoch 23, acc 0.845, loss 0.9594733428351486\n",
      "epoch 24, loss 0.9521687442559932\n",
      "saved at epoch 24, acc 0.8554, loss 0.958199986928626\n",
      "epoch 25, loss 0.9507322292338048\n",
      "epoch 26, loss 0.9500288060987427\n",
      "epoch 27, loss 0.9472325493786127\n",
      "saved at epoch 27, acc 0.8612, loss 0.9566386870191067\n",
      "epoch 28, loss 0.9464587238805888\n",
      "saved at epoch 28, acc 0.8642, loss 0.9617344774777377\n",
      "epoch 29, loss 0.9431310364686604\n",
      "saved at epoch 29, acc 0.8668, loss 0.9535193194316912\n",
      "epoch 30, loss 0.9408727497942666\n",
      "saved at epoch 30, acc 0.8672, loss 0.9506903622723832\n",
      "epoch 31, loss 0.9401555194783566\n",
      "saved at epoch 31, acc 0.8726, loss 0.9508958896504173\n",
      "epoch 32, loss 0.9395383273614749\n",
      "saved at epoch 32, acc 0.881, loss 0.9479050334495834\n",
      "epoch 33, loss 0.9373554248037115\n",
      "epoch 34, loss 0.9342299258785207\n",
      "saved at epoch 34, acc 0.8873, loss 0.9435024125666558\n",
      "epoch 35, loss 0.934738697146556\n",
      "epoch 36, loss 0.9327982331135634\n",
      "saved at epoch 36, acc 0.8939, loss 0.9450068986868556\n",
      "epoch 37, loss 0.931086803423062\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "device = torch.device('cpu')\n",
    "model = MainModel(device).to(device)\n",
    "# model.load_state_dict(torch.load('saveddict/125_25_wider.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "model.train()\n",
    "max_acc, min_loss = 0.0, 100\n",
    "for epoch in range(5000):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, patch_num, patch_size)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print('epoch {}, loss {}'.format(epoch, epoch_loss))\n",
    "\n",
    "    # test model\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(-1, patch_num, patch_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        epoch_loss /= len(test_loader)\n",
    "        \n",
    "        # save best model, both accuracy and loss\n",
    "        if correct/total > max_acc: #and loss.item() < min_loss:\n",
    "            max_acc = correct/total\n",
    "            torch.save(model.state_dict(), 'saveddict/{}x{}_{}_wider.pth'.format(patch_num, patch_out, patch_size))\n",
    "            print('saved at epoch {}, acc {}, loss {}'.format(epoch, max_acc, epoch_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
