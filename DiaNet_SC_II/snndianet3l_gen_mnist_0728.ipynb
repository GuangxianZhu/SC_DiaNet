{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "import autogendianet as autogen\n",
    "from nbformat.v4 import new_notebook, new_code_cell\n",
    "from datetime import datetime\n",
    "\n",
    "# 创建一个新的笔记本对象\n",
    "nb = new_notebook()\n",
    "\n",
    "# 获取当前日期, 转换为指定的形式(如202307311453)\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "filename = 'auto_mnist_3layer_' + formatted_time + '.ipynb'\n",
    "\n",
    "test_mode = 0\n",
    "# device_setting, 配置snn的执行位置\n",
    "auto = 'device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")'\n",
    "user = 'device = \"cpu\"'\n",
    "device_setting = auto\n",
    "\n",
    "#脚本使用的参数, 用于生成dianet拓扑和配置\n",
    "attrb_num = 256\n",
    "label_num = 10\n",
    "\n",
    "#限制突触\n",
    "w_up = 3\n",
    "w_dwn = -3\n",
    "b_up = 3\n",
    "b_dwn = -3\n",
    "\n",
    "# preprocess_dataset, 这段代码会嵌入到数据集处理的头部, 没办法, 参数太多不如单独写了\n",
    "preprocess_dataset = '''\n",
    "train_db = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                   ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_db = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_loader = torch.utils.data.DataLoader(test_db,\n",
    "    batch_size=10000, shuffle=True)\n",
    "train_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])\n",
    "#print('db1:', len(train_db), 'db2:', len(val_db))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "'''\n",
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "parameter_cell = f'''\n",
    "# snn-Dianet parameters\n",
    "# 获取当前时间\n",
    "current_time = datetime.now()\n",
    "print('Start time: ' + str(current_time))\n",
    "\n",
    "# Neuron\n",
    "step_num = 4\n",
    "thresh = 1.0 # neuronal threshold\n",
    "lens = 0.5 # hyper-parameters of approximate function\n",
    "\n",
    "# Training\n",
    "initial_lr = 0.1\n",
    "epoch_num = 150\n",
    "\n",
    "# Set the project\n",
    "batch_size = 800\n",
    "dtype = torch.float\n",
    "{device_setting}\n",
    "print(\"Message: This project will run on \" + str(device) + \". \")\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成输入的拓扑\n",
    "input_site_array, input_site_width = autogen.create_input_site(attrb_num)\n",
    "\n",
    "#_______________test the function_______________\n",
    "if test_mode:\n",
    "    print(input_site_array)\n",
    "    print(input_site_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建dianet拓扑\n",
    "dianet_neuron_array, dianet_layer_width = autogen.create_dianet_neuron_array(input_site_array, label_num)\n",
    "\n",
    "#_______________test the function_______________\n",
    "if test_mode:\n",
    "    print(dianet_neuron_array)\n",
    "    print(dianet_layer_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建dianet的属性列表\n",
    "dianet_neuron_feature = autogen.create_dianet_neuron_feature(input_site_array, dianet_neuron_array)\n",
    "\n",
    "#_______________test the function_______________\n",
    "if test_mode:\n",
    "    print(dianet_neuron_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the topolog: \n",
      "                             0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22\n",
      "                          0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23\n",
      "                       0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24\n",
      "                    0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25\n",
      "                 0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26\n",
      "              0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27\n",
      "           0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28\n",
      "        0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29\n",
      "     0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30\n",
      "  0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30    31\n",
      "     0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30\n",
      "        0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29\n",
      "           0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28\n",
      "              0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27\n",
      "                 0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26\n",
      "                    0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25\n",
      "                       0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23    24\n",
      "                          0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22    23\n",
      "                             0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22\n",
      "                                0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21\n",
      "                                   0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20\n",
      "                                      0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19\n",
      "                                         0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18\n",
      "                                            0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17\n",
      "                                               0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16\n",
      "                                                  0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15\n",
      "                                                     0     1     2     3     4     5     6     7     8     9    10    11    12    13    14\n",
      "                                                        0     1     2     3     4     5     6     7     8     9    10    11    12    13\n",
      "                                                           0     1     2     3     4     5     6     7     8     9    10    11    12\n",
      "                                                              0     1     2     3     4     5     6     7     8     9    10    11\n",
      "                                                                 0     1     2     3     4     5     6     7     8     9    10\n",
      "                                                                    0     1     2     3     4     5     6     7     8     9\n",
      "\n",
      "This is input site: \n",
      "                             0     1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    21    22\n",
      "                               23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38    39    40    41    42    43    44\n",
      "                                  45    46    47    48    49    50    51    52    53    54    55    56    57    58    59    60    61    62    63    64    65\n",
      "                                     66    67    68    69    70    71    72    73    74    75    76    77    78    79    80    81    82    83    84    85\n",
      "                                        86    87    88    89    90    91    92    93    94    95    96    97    98    99   100   101   102   103   104\n",
      "                                          105   106   107   108   109   110   111   112   113   114   115   116   117   118   119   120   121   122\n",
      "                                             123   124   125   126   127   128   129   130   131   132   133   134   135   136   137   138   139\n",
      "                                                140   141   142   143   144   145   146   147   148   149   150   151   152   153   154   155\n",
      "                                                   156   157   158   159   160   161   162   163   164   165   166   167   168   169   170\n",
      "                                                      171   172   173   174   175   176   177   178   179   180   181   182   183   184\n",
      "                                                         185   186   187   188   189   190   191   192   193   194   195   196   197\n",
      "                                                            198   199   200   201   202   203   204   205   206   207   208   209\n",
      "                                                               210   211   212   213   214   215   216   217   218   219   220\n",
      "                                                                  221   222   223   224   225   226   227   228   229   230\n",
      "                                                                     231   232   233   234   235   236   237   238   239\n",
      "                                                                        240   241   242   243   244   245   246   247\n",
      "                                                                           248   249   250   251   252   253   254\n",
      "                                                                              255\n"
     ]
    }
   ],
   "source": [
    "#_______________test the function_______________\n",
    "if test_mode:\n",
    "    autogen.print_dianet_topolog(input_site_array, dianet_neuron_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_______________test the function_______________\n",
    "if test_mode:\n",
    "    autogen.print_dianet_neuron_feature(dianet_neuron_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新的代码单元格，并设置其内容\n",
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "improt_cell = '''\n",
    "# Imports\n",
    "\n",
    "    #避免出现vscode下plt工作不正常\n",
    "import os \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"   \n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "    # 解除打印tensor尺寸的限制\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "# torch.set_printoptions(profile=\"default\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________________________________cell body_______________________________________\n",
    "organize_dataset_cell = f'''\n",
    "# Organize dataset\n",
    "\n",
    "{preprocess_dataset}\n",
    "\n",
    "\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def gen_upper_3_layer_evtmask(this_layer_num, upper_3_layer_num):\n",
    "    original_list = []\n",
    "    for i in range(this_layer_num):\n",
    "        row = [0] * (i) + [1] * 4 + [0] * (this_layer_num - i - 1)\n",
    "        original_list.append(row)\n",
    "\n",
    "    if (upper_3_layer_num - this_layer_num) == -3:      #ex-ex-ex-ex/max, 各删3列\n",
    "        upper_3_layer_evtmask = [row[3:-3] for row in original_list]\n",
    "\n",
    "    elif (upper_3_layer_num - this_layer_num) == -1:    #ex-ex-max-cp, 各删2列\n",
    "        upper_3_layer_evtmask = [row[2:-2] for row in original_list]\n",
    "\n",
    "    elif (upper_3_layer_num - this_layer_num) == 1:    #ex-max-cp-cp, 各删1列\n",
    "        upper_3_layer_evtmask = [row[1:-1] for row in original_list]\n",
    "\n",
    "    elif (upper_3_layer_num - this_layer_num) == 3:    #max/cp-cp-cp-cp, 不需处理\n",
    "        upper_3_layer_evtmask = original_list\n",
    "\n",
    "    else:\n",
    "        print(\"Sth error!\")\n",
    "\n",
    "    return upper_3_layer_evtmask\n",
    "\n",
    "if test_mode:\n",
    "    this_layer_num = 9  # 修改这里的值来改变列表的行数\n",
    "    upper_3_layer_num = 12  # 修改这里的值来改变列表的列数\n",
    "    generated_list = gen_upper_3_layer_evtmask(this_layer_num, upper_3_layer_num)\n",
    "    for row in generated_list:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def gen_upper_2_layer_evtmask(this_layer_num, upper_2_layer_num):\n",
    "    original_list = []\n",
    "    for i in range(this_layer_num):\n",
    "        row = [0] * (i) + [1] * 3 + [0] * (this_layer_num - i - 1)\n",
    "        original_list.append(row)\n",
    "\n",
    "    if (upper_2_layer_num - this_layer_num) == -2:      #ex-ex-ex-ex/max, 各删2列\n",
    "        upper_2_layer_evtmask = [row[2:-2] for row in original_list]\n",
    "\n",
    "    elif (upper_2_layer_num - this_layer_num) == 0:    #ex-ex-max-cp, 各删1列\n",
    "        upper_2_layer_evtmask = [row[1:-1] for row in original_list]\n",
    "\n",
    "    elif (upper_2_layer_num - this_layer_num) == 2:    #max/cp-cp-cp-cp, 不需处理\n",
    "        upper_2_layer_evtmask = original_list\n",
    "\n",
    "    else:\n",
    "        print(\"Sth error!\")\n",
    "        \n",
    "    return upper_2_layer_evtmask\n",
    "\n",
    "if test_mode:\n",
    "    this_layer_num = 11  # 修改这里的值来改变列表的行数\n",
    "    upper_2_layer_num = 11  # 修改这里的值来改变列表的列数\n",
    "    generated_list = gen_upper_2_layer_evtmask(this_layer_num, upper_2_layer_num)\n",
    "    for row in generated_list:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隐藏层的层数, 从拓扑获取\n",
    "hid_layer_num = len(dianet_neuron_array) - 2\n",
    "\n",
    "\n",
    "#生成linear, mask___________________________________________________________________________________\n",
    "create_fc_nrm_msk = ''  #保存生成代码的字符串, 直接嵌入f{}中\n",
    "tab_pad = 4 * ' ' * 2\n",
    "\n",
    "for i in range(1, len(dianet_neuron_array)):    #从第[1]行神经元开始遍历, 因为第[0]行是纯输入, 处理方式不同\n",
    "    upper_layer_neuron_num = len(dianet_neuron_array[i-1])\n",
    "    input_site_num = 0\n",
    "    for j, neuron_feature in enumerate(dianet_neuron_feature[i]):\n",
    "        if neuron_feature['tap_i'] != None:\n",
    "            input_site_num += 1\n",
    "        \n",
    "    fc_in_num = upper_layer_neuron_num + input_site_num    #本行神经元一共包含多少个输入, 包含上一层神经元数与本层额外输入\n",
    "\n",
    "    fc_msk_array = []   #创建本行的空白模板, 二维的, [列表行数]对应[神经元个数], [每行的成员数]对应[输入个数]\n",
    "    base_idx = 0        #用于将二维列表中, 每一行开始改1的基地址记忆下来, dianet的mask均为[多个0, 连续的2-3个1, 多个零]这种形式\n",
    "    for j, neuron_feature in enumerate(dianet_neuron_feature[i]):\n",
    "        temp_fc_msk_unit = [0] * fc_in_num\n",
    "        if neuron_feature['left_i'] != None:\n",
    "            temp_fc_msk_unit[base_idx] = 1\n",
    "            base_idx += 1\n",
    "        if neuron_feature['tap_i'] != None:\n",
    "            temp_fc_msk_unit[base_idx] = 1\n",
    "            base_idx += 1\n",
    "        if neuron_feature['right_i'] != None:\n",
    "            temp_fc_msk_unit[base_idx] = 1\n",
    "            base_idx += 1\n",
    "\n",
    "        base_idx -= 1 #补偿机制, 因为当前神经元的右输入, 和下一个神经元的左输入, 都来源于上一层夹在中间的那个神经元, 因此基地址计数器要倒退一格\n",
    "        \n",
    "        fc_msk_array.append(temp_fc_msk_unit)\n",
    "        #print(temp_fc_msk_unit)\n",
    "\n",
    "    create_fc_nrm_msk += (f'{tab_pad}self.fc{i} = nn.Linear({str(fc_in_num)}, {len(dianet_neuron_array[i])})\\n')\n",
    "    create_fc_nrm_msk += (f'{tab_pad}self.bn{i} = nn.BatchNorm1d(num_features={len(dianet_neuron_array[i])})\\n')\n",
    "    create_fc_nrm_msk += (f'{tab_pad}self.msk{i} = np.array({fc_msk_array})\\n\\n')\n",
    "\n",
    "    if i == 3:\n",
    "        upper_2_layer_evtmask = gen_upper_2_layer_evtmask(len(dianet_neuron_array[i]), len(dianet_neuron_array[i-2]))\n",
    "\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.fc{i}evt = nn.Linear({len(dianet_neuron_array[i-2])}, {len(dianet_neuron_array[i])}, bias=False)\\n')\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.bn{i}evt = nn.BatchNorm1d(num_features={len(dianet_neuron_array[i])})\\n')\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.msk{i}evt = np.array({upper_2_layer_evtmask})\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "    elif i > 3:\n",
    "        upper_3_layer_evtmask = gen_upper_3_layer_evtmask(len(dianet_neuron_array[i]), len(dianet_neuron_array[i-3]))\n",
    "        upper_2_layer_evtmask = gen_upper_2_layer_evtmask(len(dianet_neuron_array[i]), len(dianet_neuron_array[i-2]))\n",
    "        upper_3_2_layer_evtmask = [upper_3_layer_row + upper_2_layer_row for upper_3_layer_row, upper_2_layer_row in zip(upper_3_layer_evtmask, upper_2_layer_evtmask)]\n",
    "\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.fc{i}evt = nn.Linear({len(dianet_neuron_array[i-3]) + len(dianet_neuron_array[i-2])}, {len(dianet_neuron_array[i])}, bias=False)\\n')\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.bn{i}evt = nn.BatchNorm1d(num_features={len(dianet_neuron_array[i])})\\n')\n",
    "        create_fc_nrm_msk += (f'{tab_pad}self.msk{i}evt = np.array({upper_3_2_layer_evtmask})\\n\\n')\n",
    "\n",
    "\n",
    "create_fc_nrm_msk += '#auto_gen'\n",
    "\n",
    "\n",
    "#create_neuron_layer = ''\n",
    "#tab_pad = 4 * ' ' * 2   # 生成格式化空格, 虽然叫tab, 实际是填4个半角空格\n",
    "#for i in range(1, hid_layer_num + 1):\n",
    "#    create_neuron_layer += (f'{tab_pad}self.hid{i} = snn.Leaky(threshold={threshold}, beta=beta)\\n')\n",
    "#\n",
    "#create_neuron_layer += f'{tab_pad}self.out = snn.Leaky(threshold={threshold}, beta=beta)\\n'\n",
    "#create_neuron_layer += '#auto_gen'\n",
    "\n",
    "\n",
    "#初始化mem和spk状态___________________________________________________________________________________\n",
    "init_neuron_mem_spk = ''\n",
    "tab_pad = 4 * ' ' * 2\n",
    "init_neuron_mem_spk += f'{tab_pad}batch_size = pre_x.size(0)\\n'\n",
    "\n",
    "init_neuron_mem_spk += f'{tab_pad}mem_conv1 = spk_conv1 = torch.zeros(batch_size, 16, 26, 26, device=device)\\n'\n",
    "init_neuron_mem_spk += f'{tab_pad}mem_conv2 = spk_conv2 = torch.zeros(batch_size, 16, 9, 9, device=device)\\n'\n",
    "\n",
    "for i in range(1, hid_layer_num + 1):\n",
    "    init_neuron_mem_spk += (f'{tab_pad}mem_hid{i} = spk_hid{i} = torch.zeros(batch_size, {len(dianet_neuron_array[i])}, device=device)\\n')\n",
    "\n",
    "init_neuron_mem_spk += f'{tab_pad}mem_out = spk_out = spksum_out = torch.zeros(batch_size, {len(dianet_neuron_array[-1])}, device=device)\\n\\n'\n",
    "init_neuron_mem_spk += '#auto_gen'\n",
    "\n",
    "\n",
    "#创建神经元的输入输出记录___________________________________________________________________________________\n",
    "create_log_rec = ''\n",
    "tab_pad = 4 * ' ' * 2\n",
    "for i in range(1, hid_layer_num + 1):\n",
    "    create_log_rec += (f'{tab_pad}self.spk_hid{i}_rec = []\\n')\n",
    "    create_log_rec += (f'{tab_pad}self.mem_hid{i}_rec = []\\n')\n",
    "\n",
    "create_log_rec += f'{tab_pad}self.spk_out_rec = []\\n'\n",
    "create_log_rec += f'{tab_pad}self.mem_out_rec = []\\n'\n",
    "create_log_rec += '#auto_gen'\n",
    "\n",
    "\n",
    "#主迭代体, 包含各层的传输____________________________________________________________________________\n",
    "dianet_iterate = ''\n",
    "tab_pad = 4 * ' ' * 3\n",
    "\n",
    "hid1_tap_num = 0\n",
    "for j, neuron_feature in enumerate(dianet_neuron_feature[1]):\n",
    "    if (neuron_feature['tap_i'] != None):\n",
    "        hid1_tap_num += 1\n",
    "\n",
    "spk_to_hid1 = len(input_site_array[0]) + hid1_tap_num\n",
    "dianet_iterate += f'{tab_pad}spk_to_hid1 = x[:,0:{spk_to_hid1}]\\n\\n'\n",
    "\n",
    "\n",
    "tab_pad = 4 * ' ' * 3\n",
    "for i in range(1, len(dianet_neuron_array)):\n",
    "    upper_layer_neuron_num = len(dianet_neuron_array[i-1])\n",
    "    input_site_num = 0\n",
    "    for j, neuron_feature in enumerate(dianet_neuron_feature[i]):\n",
    "        if neuron_feature['tap_i'] != None:\n",
    "            input_site_num += 1\n",
    "        \n",
    "    fc_in_num = upper_layer_neuron_num + input_site_num\n",
    "\n",
    "    if i < (len(dianet_neuron_array) - 2):\n",
    "        netlist_array = []\n",
    "        for j, neuron_feature in enumerate(dianet_neuron_feature[i+1]):\n",
    "            if (neuron_feature['left_i'] != None) and (len(netlist_array) == 0):\n",
    "                netlist_array.append(neuron_feature['left_i'][0])\n",
    "\n",
    "            if (neuron_feature['tap_i'] != None):\n",
    "                netlist_array.append(neuron_feature['tap_i'])\n",
    "\n",
    "            if (neuron_feature['right_i'] != None):\n",
    "                netlist_array.append(neuron_feature['right_i'][0])\n",
    "\n",
    "        #print(netlist_array)\n",
    "        netlist_str = ''\n",
    "        for item in netlist_array:\n",
    "            if isinstance(item, list):\n",
    "                netlist_str += f'spk_hid{item[0]}_pth[:, {item[1]}:{item[1]+1}], '\n",
    "                \n",
    "            elif isinstance(item, int):\n",
    "                netlist_str += f'x[:, {item}:{item+1}], '\n",
    "            else:\n",
    "                print('Detected an illegal item!')\n",
    "\n",
    "        hid_with_patch = ''\n",
    "        if i < 3:\n",
    "            hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i}\\n'\n",
    "\n",
    "        else:\n",
    "            hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i}\\n'\n",
    "\n",
    "#            if len(dianet_neuron_array[i]) > len(dianet_neuron_array[i-2]):\n",
    "#                hid_with_patch += f'{tab_pad}spk_hid{i}_pth = torch.cat((spk_hid{i}[:, 0:1], spk_hid{i-2}_pth + spk_hid{i}[:,1:{len(dianet_neuron_array[i])-1}], spk_hid{i}[:, {len(dianet_neuron_array[i])-1}:{len(dianet_neuron_array[i])}]), dim=1)\\n'\n",
    "#            elif len(dianet_neuron_array[i]) == len(dianet_neuron_array[i-2]):\n",
    "#                hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i-2}_pth + spk_hid{i}\\n'\n",
    "#            elif len(dianet_neuron_array[i]) < len(dianet_neuron_array[i-2]):\n",
    "#                hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i-2}_pth[:,1:{len(dianet_neuron_array[i-2])-1}] + spk_hid{i}\\n'\n",
    "#            else:\n",
    "#                hid_with_patch = '\\n'\n",
    "\n",
    "        cur_to_layer = ''\n",
    "        if i < 3:\n",
    "            cur_to_layer = f'{tab_pad}cur_to_hid{i} = self.bn{i}(self.fc{i}(spk_to_hid{i}))\\n'\n",
    "\n",
    "        elif i == 3:\n",
    "            cur_to_layer = f'{tab_pad}cur_to_hid{i} = self.bn{i}(self.fc{i}(spk_to_hid{i})) + self.bn{i}evt(self.fc{i}evt(spk_hid{i-2}))\\n'\n",
    "\n",
    "        else:\n",
    "            cur_to_layer = f'{tab_pad}cur_to_hid{i} = self.bn{i}(self.fc{i}(spk_to_hid{i})) + self.bn{i}evt(self.fc{i}evt(torch.cat((spk_hid{i-3}, spk_hid{i-2}), 1)))\\n'  \n",
    "\n",
    "        dianet_iterate += f'{tab_pad}self.fc{i}.weight.data *= torch.from_numpy(self.msk{i}).float().to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.weight.data = torch.clamp(self.fc{i}.weight.data, min=w_dwn, max=w_up).to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.bias.data = torch.clamp(self.fc{i}.bias.data, min=b_dwn, max=b_up).to(device)\\n'\n",
    "        dianet_iterate += cur_to_layer\n",
    "        dianet_iterate += f'{tab_pad}mem_hid{i}, spk_hid{i} = mem_update(cur_to_hid{i}, mem_hid{i}, spk_hid{i})\\n'\n",
    "        dianet_iterate += hid_with_patch\n",
    "        dianet_iterate += f'{tab_pad}spk_to_hid{i+1} = torch.cat(({netlist_str[:-2]}), 1)\\n\\n'\n",
    "\n",
    "        \n",
    "\n",
    "    elif i == (len(dianet_neuron_array) - 2):\n",
    "        netlist_array = []\n",
    "        for j, neuron_feature in enumerate(dianet_neuron_feature[i+1]):\n",
    "            if (neuron_feature['left_i'] != None) and (len(netlist_array) == 0):\n",
    "                netlist_array.append(neuron_feature['left_i'][0])\n",
    "\n",
    "            if (neuron_feature['tap_i'] != None):\n",
    "                netlist_array.append(neuron_feature['tap_i'])\n",
    "\n",
    "            if (neuron_feature['right_i'] != None):\n",
    "                netlist_array.append(neuron_feature['right_i'][0])\n",
    "        \n",
    "        #print(netlist_array)\n",
    "        netlist_str = ''\n",
    "        for item in netlist_array:\n",
    "            if isinstance(item, list):\n",
    "                netlist_str += f'spk_hid{item[0]}_pth[:, {item[1]}:{item[1]+1}], '\n",
    "                \n",
    "            elif isinstance(item, int):\n",
    "                netlist_str += f'x[:, {item}:{item+1}], '\n",
    "            else:\n",
    "                print('Detected an illegal item!')\n",
    "\n",
    "        hid_with_patch = ''\n",
    "        if i < 3:\n",
    "            hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i}\\n'\n",
    "        \n",
    "        elif i >= 3:\n",
    "            if len(dianet_neuron_array[i]) > len(dianet_neuron_array[i-2]):\n",
    "                hid_with_patch += f'{tab_pad}spk_hid{i}_pth = torch.cat((spk_hid{i}[:, 0:1], spk_hid{i-2}_pth + spk_hid{i}[:,1:{len(dianet_neuron_array[i])-1}], spk_hid{i}[:, {len(dianet_neuron_array[i])-1}:{len(dianet_neuron_array[i])}]), dim=1)\\n'\n",
    "            elif len(dianet_neuron_array[i]) == len(dianet_neuron_array[i-2]):\n",
    "                hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i-2}_pth + spk_hid{i}\\n'\n",
    "            elif len(dianet_neuron_array[i]) < len(dianet_neuron_array[i-2]):\n",
    "                hid_with_patch = f'{tab_pad}spk_hid{i}_pth = spk_hid{i-2}_pth[:,1:{len(dianet_neuron_array[i-2])-1}] + spk_hid{i}\\n'\n",
    "            else:\n",
    "                hid_with_patch = '\\n'\n",
    "\n",
    "        dianet_iterate += f'{tab_pad}self.fc{i}.weight.data *= torch.from_numpy(self.msk{i}).float().to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.weight.data = torch.clamp(self.fc{i}.weight.data, min=w_dwn, max=w_up).to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.bias.data = torch.clamp(self.fc{i}.bias.data, min=b_dwn, max=b_up).to(device)\\n'\n",
    "        dianet_iterate += f'{tab_pad}cur_to_hid{i} = self.bn{i}(self.fc{i}(spk_to_hid{i})) + self.bn{i}evt(self.fc{i}evt(torch.cat((spk_hid{i-3}, spk_hid{i-2}), 1)))\\n' \n",
    "        dianet_iterate += f'{tab_pad}mem_hid{i}, spk_hid{i} = mem_update(cur_to_hid{i}, mem_hid{i}, spk_hid{i})\\n'\n",
    "        dianet_iterate += hid_with_patch\n",
    "        dianet_iterate += f'{tab_pad}spk_to_out = torch.cat(({netlist_str[:-2]}), 1)\\n\\n'\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        dianet_iterate += f'{tab_pad}self.fc{i}.weight.data *= torch.from_numpy(self.msk{i}).float().to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.weight.data = torch.clamp(self.fc{i}.weight.data, min=w_dwn, max=w_up).to(device)\\n'\n",
    "        #dianet_iterate += f'{tab_pad}#self.fc{i}.bias.data = torch.clamp(self.fc{i}.bias.data, min=b_dwn, max=b_up).to(device)\\n'\n",
    "        dianet_iterate += f'{tab_pad}cur_to_out = self.bn{i}(self.fc{i}(spk_to_out)) + self.bn{i}evt(self.fc{i}evt(torch.cat((spk_hid{i-3}, spk_hid{i-2}), 1)))\\n' \n",
    "        dianet_iterate += f'{tab_pad}mem_out, spk_out = mem_update(cur_to_out, mem_out, spk_out)\\n\\n'\n",
    "\n",
    "dianet_iterate += '#auto_gen'\n",
    "\n",
    "\n",
    "\n",
    "save_spk_mem_rec = ''\n",
    "tab_pad = 4 * ' ' * 3\n",
    "for i in range(1, hid_layer_num + 1):\n",
    "    save_spk_mem_rec += (f'{tab_pad}self.spk_hid{i}_rec.append(spk_hid{i})\\n')\n",
    "    save_spk_mem_rec += (f'{tab_pad}self.mem_hid{i}_rec.append(mem_hid{i})\\n')\n",
    "\n",
    "save_spk_mem_rec += f'{tab_pad}self.spk_out_rec.append(spk_out)\\n'\n",
    "save_spk_mem_rec += f'{tab_pad}self.mem_out_rec.append(mem_out)\\n'\n",
    "save_spk_mem_rec += '#auto_gen'\n",
    "\n",
    "\n",
    "\n",
    "print_spk_log = ''\n",
    "tab_pad = 4 * ' ' * 4\n",
    "for i in range(1, hid_layer_num + 1):\n",
    "    print_spk_log += (f'{tab_pad}print(\" spk_hid{i}:\\t|\", [float(num) for num in self.spk_hid{i}_rec[step][batch].tolist()])\\n')\n",
    "\n",
    "print_spk_log += f'{tab_pad}print(\" spk_out:\\t|\", [float(num) for num in self.spk_out_rec[step][batch].tolist()])\\n'\n",
    "print_spk_log += '#auto_gen'\n",
    "\n",
    "print_mem_log = ''\n",
    "tab_pad = 4 * ' ' * 4\n",
    "for i in range(1, hid_layer_num + 1):\n",
    "    print_mem_log += (f'{tab_pad}print(\" mem_hid{i}:\\t|\", [float(num) for num in self.mem_hid{i}_rec[step][batch].tolist()])\\n')\n",
    "\n",
    "print_mem_log += f'{tab_pad}print(\" mem_out:\\t|\", [float(num) for num in self.mem_out_rec[step][batch].tolist()])\\n'\n",
    "print_mem_log += '#auto_gen'\n",
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "create_dianet_cell = f'''\n",
    "\n",
    "class Actfun(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(thresh).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        #temp = abs(input - thresh) < lens\n",
    "        #temp=1/(1+torch.exp(-(input-0)))\n",
    "        temp = torch.exp( -(input - thresh) **2/(2 * lens ** 2) ) / ((2 * lens * 3.141592653589793) ** 0.5) \n",
    "        return grad_input * temp.float()\n",
    "\n",
    "actfun = Actfun.apply\n",
    "# membrane potential update\n",
    "def mem_update(x, mem, spike):\n",
    "    mem = mem *(1. - spike) + x\n",
    "    spike = actfun(mem) # actfun : approximation firing function\n",
    "    return mem, spike\n",
    "\n",
    "\n",
    "# Create Dianet\n",
    "\n",
    "class Dianet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dianet, self).__init__()\n",
    "        \n",
    "        #mnist所需的卷积操作, 压缩输入维度\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 0)\n",
    "        self.bnconv1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool1 = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 5)\n",
    "        self.bnconv2 = nn.BatchNorm2d(16)\n",
    "        self.maxpool2 = nn.MaxPool2d(2,stride=2)\n",
    "        \n",
    "        #上一层有几个神经元+输入抽头+跳层输入, mask里的每一项就有几个\n",
    "        #本层有几个神经元, mask里就有几项\n",
    "{create_fc_nrm_msk}\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, pre_x, w_dwn = {w_dwn}, w_up = {w_up}, b_dwn = {b_dwn}, b_up = {b_up}):\n",
    "\n",
    "        # Initialize hidden states at t=0 时间相关的神经元, 每轮模拟开始时初始化LIF状态\n",
    "{init_neuron_mem_spk}\n",
    "\n",
    "\n",
    "        # Record the final layer 收集脉冲和膜电位信息\n",
    "{create_log_rec}\n",
    "\n",
    "\n",
    "\n",
    "        for step in range(step_num):\n",
    "        # input → hid1    \n",
    "            spk_to_conv1 = pre_x > torch.rand(pre_x.size(), device=device)\n",
    "            cur_to_conv1 = self.bnconv1(self.conv1(spk_to_conv1.float()))\n",
    "            mem_conv1, spk_conv1 = mem_update(cur_to_conv1, mem_conv1, spk_conv1)\n",
    "            spk_to_conv2 = self.maxpool1(spk_conv1)\n",
    "            cur_to_conv2 = self.bnconv2(self.conv2(spk_to_conv2))\n",
    "            mem_conv2, spk_conv2 = mem_update(cur_to_conv2, mem_conv2, spk_conv2)\n",
    "            convout = self.maxpool2(spk_conv2) \n",
    "            x = convout.view(convout.size()[0], -1)\n",
    "\n",
    "            #本质上是模拟输入, 并非真实的脉冲\n",
    "{dianet_iterate}\n",
    "            \n",
    "            #保存信息\n",
    "{save_spk_mem_rec}\n",
    "            \n",
    "            \n",
    "            spksum_out += mem_out\n",
    "        spk_out_avg = spksum_out / step_num\n",
    "        return spk_out_avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def print_spk_info(self):\n",
    "        print(\"Spike at each time step:\")\n",
    "        for batch in range(0, 1):\n",
    "            print(f\"Batch num {{batch}}:\")\n",
    "            for step in range(step_num):\n",
    "                print(f\"Time step {{step}}:\")\n",
    "{print_spk_log}\n",
    "\n",
    "\n",
    "\n",
    "    def print_mem_info(self):\n",
    "        print(\"Mem at each time step:\")\n",
    "        for batch in range(0, 1):\n",
    "            print(f\"Batch num {{batch}}:\")\n",
    "            for step in range(step_num):\n",
    "                print(f\"Time step {{step}}:\")\n",
    "{print_mem_log}\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "load_snn_cell = '''\n",
    "# Load the network onto device\n",
    "net = Dianet().to(device)\n",
    "\n",
    "criteon = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=initial_lr, weight_decay=0.00001,betas=(0.9, 0.99))\n",
    "#optimizer = optim.Adam(net.parameters(), lr=initial_lr,weight_decay=0.00001,betas=(0.9, 0.99))\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "training_cell = '''\n",
    "# Training\n",
    "\n",
    "def adjust_learning_rate(epoch_num):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = initial_lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr \n",
    "\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    adjust_learning_rate(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        net.train()\n",
    "        logits = net(data)\n",
    "        loss = criteon(logits, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       20. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        logits = net(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "        test_loss_hist.append(test_loss)\n",
    "        \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    print('\\\\nVAL set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\\\n'.format(\n",
    "        test_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "\n",
    "        \n",
    "# 获取当前时间\n",
    "current_time = datetime.now()\n",
    "print('Start time: ' + str(current_time))\n",
    "\n",
    "\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________________________________cell body_______________________________________\n",
    "draw_loss_cell = '''\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(torch.tensor(loss_hist).cpu())\n",
    "plt.plot(torch.tensor(test_loss_hist).cpu())\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________________________________cell body_______________________________________\n",
    "run_testset_cell = '''\n",
    "# Accuracy\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "#test_loader = DataLoader(wine_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    logits = net(data)\n",
    "    test_loss += criteon(logits, target).item()\n",
    "\n",
    "    pred = logits.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "    #print(data)\n",
    "    #print('tar: ' + str(targets))\n",
    "    #print('prd: ' + str(predicted))\n",
    "    #print(test_spk)\n",
    "    #net.print_spk_info()\n",
    "    net.print_mem_info()\n",
    "    \n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\\\nVAL set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "'''\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_info = ''\n",
    "\n",
    "for i in range(1, len(dianet_neuron_array)):    #从第[1]行神经元开始遍历, 因为第[0]行是纯输入, 处理方式不同\n",
    "    print_layer_info += f'print(net.fc{i}.weight.data)\\n'\n",
    "    print_layer_info += f'print(net.fc{i}.bias.data)\\n\\n'\n",
    "\n",
    "#____________________________________________cell body_______________________________________\n",
    "other_cell = f'''\n",
    "\n",
    "{print_layer_info}\n",
    "\n",
    "'''\n",
    "\n",
    "#____________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_cell = f'''\n",
    "\n",
    "# Save the model\n",
    "\n",
    "# 获取当前时间, 转换为指定的形式(如202307311453)\n",
    "current_time = datetime.now()\n",
    "formatted_datetime = current_time.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "torch.save(net.state_dict(), formatted_datetime + '_snndianet_model_checkpoint.pth')\n",
    "torch.save(net, formatted_datetime + '_snndianet_full_model_checkpoint.pth')\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improt_cell, 程序必备的包\n",
    "nb.cells.append(new_code_cell(source=improt_cell[1:]))\n",
    "\n",
    "# parameter_cell, 对程序进行配置\n",
    "nb.cells.append(new_code_cell(source=parameter_cell[1:]))\n",
    "\n",
    "# organize_dataset_cell, 对数据集进行处理\n",
    "nb.cells.append(new_code_cell(source=organize_dataset_cell[1:]))\n",
    "\n",
    "# create_dianet_cell, 利用脚本自动生成dianet\n",
    "nb.cells.append(new_code_cell(source=create_dianet_cell[1:]))\n",
    "\n",
    "# load_snn_cell, 将模型加载到device\n",
    "nb.cells.append(new_code_cell(source=load_snn_cell[1:]))\n",
    "\n",
    "# training_cell, 完成模型的训练\n",
    "nb.cells.append(new_code_cell(source=training_cell[1:]))\n",
    "\n",
    "# save_model_cell, 保存训练后的模型\n",
    "nb.cells.append(new_code_cell(source=save_model_cell[1:]))\n",
    "\n",
    "# draw_loss_cell, 绘制loss图\n",
    "nb.cells.append(new_code_cell(source=draw_loss_cell[1:]))\n",
    "\n",
    "# run_testset_cell, 执行测试集\n",
    "nb.cells.append(new_code_cell(source=run_testset_cell[1:]))\n",
    "\n",
    "# 将other_cell添加到笔记本中\n",
    "nb.cells.append(new_code_cell(source=other_cell[1:]))\n",
    "\n",
    "# 将笔记本保存为.ipynb文件\n",
    "nbf.write(nb, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
